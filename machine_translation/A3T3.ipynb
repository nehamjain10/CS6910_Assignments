{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pOFGzRcpSuwG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import re\n",
        "import pickle\n",
        "from torchtext.vocab import vocab\n",
        "import string\n",
        "import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIfcXfdOrXFR",
        "outputId": "987b9ea9-3c6f-48ec-9765-1b7d017dad8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing AlbertModel: ['sop_classifier.classifier.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'sop_classifier.classifier.weight', 'predictions.bias', 'predictions.LayerNorm.weight']\n",
            "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "gu_tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "model = AutoModel.from_pretrained('ai4bharat/indic-bert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_en_vocab():\n",
        "    counter = Counter()\n",
        "    for fp in [train_data,val_data,test_data]:\n",
        "        for i in fp:\n",
        "            counter.update(i[1])\n",
        "    return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WAee0ZaPiIpK"
      },
      "outputs": [],
      "source": [
        "train_filepaths = ['en-gu/train.en', 'en-gu/train.gu']\n",
        "val_filepaths = ['en-gu/dev.en', 'en-gu/dev.gu']\n",
        "test_filepaths = ['en-gu/test.en', 'en-gu/test.gu']\n",
        "\n",
        "# Loading vocab to embedding converter of indicbert\n",
        "vocab_to_embedding_convertor = model.get_input_embeddings()\n",
        "\n",
        "# Tokenizer for english words\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6DUi2TMgNw_w"
      },
      "outputs": [],
      "source": [
        "file = open(\"train_data.obj\",'rb')\n",
        "train_data = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "file = open(\"val_data.obj\",'rb')\n",
        "val_data = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "file = open(\"test_data.obj\",'rb')\n",
        "test_data = pickle.load(file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "en_vocab = build_en_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_vocab[\"<pad>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[2, 0, 3]], 'token_type_ids': [[0, 0, 0]], 'attention_mask': [[1, 1, 1]]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gu_tokenizer([\"<pad>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "200000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gu_tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_embeddings = torchtext.vocab.GloVe(name='6B', dim=300)\n",
        "itos = en_vocab.get_itos() \n",
        "\n",
        "en_embeddings = []\n",
        "for i in range(len(itos)):\n",
        "    en_embeddings.append(glove_embeddings.get_vecs_by_tokens(itos[i], lower_case_backup=True).numpy())\n",
        "\n",
        "en_embeddings = np.array(en_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_to_embedding_convertor = model.get_input_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bjCPLCs96rP2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "max_length = 40\n",
        "def generate_batch(data_batch):\n",
        "  gu_batch_embeddings, en_batch_tokens, gu_tokens = [], [], []\n",
        "  for (gu_item, en_item, _) in data_batch:\n",
        "    # Token to embedding for gujarati\n",
        "    gu_embeddings = vocab_to_embedding_convertor(torch.tensor(gu_item))    \n",
        "    gu_batch_embeddings.append(gu_embeddings)\n",
        "    gu_tokens.append(torch.tensor(gu_item))\n",
        "    en_tokens = torch.tensor(en_vocab(en_item))\n",
        "    en_batch_tokens.append(en_tokens)\n",
        "    \n",
        "  gu_batch_embeddings = pad_sequence(gu_batch_embeddings,batch_first=True,padding_value=0)\n",
        "  gu_tokens = pad_sequence(gu_tokens,batch_first=True,padding_value=0)\n",
        "  en_batch_tokens = pad_sequence(en_batch_tokens,batch_first=True,padding_value=1)\n",
        "  \n",
        "  if gu_tokens.shape[1] > max_length:\n",
        "    gu_tokens = gu_tokens[:,:max_length]\n",
        "    gu_batch_embeddings = gu_batch_embeddings[:,:max_length,:]\n",
        "  return en_batch_tokens, gu_batch_embeddings, gu_tokens\n",
        "\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "n7Coeh08C2by"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_dim,embeddings,\n",
        "                 enc_hid_dim):\n",
        "        super(Encoder,self).__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float().to(device))\n",
        "        self.lstm = nn.LSTM(self.emb_dim, self.enc_hid_dim,1,batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)\n",
        "        outputs, (hidden,_) = self.lstm(src)\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_dim,\n",
        "                 dec_hid_dim):\n",
        "        super(Decoder,self).__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = gu_tokenizer.vocab_size\n",
        "        \n",
        "        self.lstm = nn.LSTM(self.emb_dim, self.dec_hid_dim,1,batch_first=True)\n",
        "        self.linear = nn.Linear(self.dec_hid_dim, self.output_dim)\n",
        "        self.translated_sentence = []\n",
        "\n",
        "    def forward(self, input, hidden) :\n",
        "        cell = torch.zeros_like(hidden)\n",
        "        outputs, (_,_) = self.lstm(input,(hidden,cell))\n",
        "        outputs = self.linear(outputs)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kqu9KuCHCRT",
        "outputId": "379a13b8-b96c-4fd8-f003-cbf79ecbdad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 33,443,584 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "ENC_EMB_DIM = 300\n",
        "ENC_HID_DIM = 256\n",
        "\n",
        "DEC_EMB_DIM = 128\n",
        "DEC_HID_DIM = 256\n",
        "\n",
        "enc = Encoder(ENC_EMB_DIM,en_embeddings,ENC_HID_DIM)\n",
        "\n",
        "dec = Decoder(DEC_EMB_DIM, DEC_HID_DIM)\n",
        "\n",
        "optimizer = optim.Adam(list(enc.parameters())+list(dec.parameters()))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oH5zNIgDI78j"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc = enc.to(device)\n",
        "dec = dec.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "G3zcjc2vJA1B",
        "outputId": "8d93734d-6ae1-4dae-ddac-5064a2a2d14a"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 1.86 GiB (GPU 0; 10.92 GiB total capacity; 6.62 GiB already allocated; 374.50 MiB free; 7.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8cb00adc91e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8cb00adc91e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/seathru/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/seathru/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.86 GiB (GPU 0; 10.92 GiB total capacity; 6.62 GiB already allocated; 374.50 MiB free; 7.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def train(iterator, optimizer, criterion):\n",
        "    enc.train()\n",
        "    dec.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, (src, trg, trg_tokens) in enumerate(iterator):\n",
        "    \n",
        "        src, trg, trg_tokens = src.to(device), trg.to(device), trg_tokens.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, hidden = enc(src)\n",
        "        output = dec(trg[:,:-1], hidden)\n",
        "        output = output.permute(0,2,1)\n",
        "        \n",
        "        #_, predicted = output.max(2)\n",
        "        loss = criterion(output, trg_tokens[:,1:])\n",
        "\n",
        "        enc.zero_grad()\n",
        "        dec.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(iterator, criterion):\n",
        "    enc.eval()\n",
        "    dec.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for _, (src, trg, trg_tokens) in enumerate(iterator):\n",
        "        \n",
        "            src, trg, trg_tokens = src.to(device), trg.to(device), trg_tokens.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, hidden = enc(src)\n",
        "            output = dec(trg[:,:-1], hidden)\n",
        "            output = output.permute(0,2,1)\n",
        "            \n",
        "            #_, predicted = output.max(2)\n",
        "            loss = criterion(output, trg_tokens[:,1:])\n",
        "\n",
        "            enc.zero_grad()\n",
        "            dec.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(train_iter, optimizer, criterion)\n",
        "    valid_loss = evaluate(valid_iter, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "A3T3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
