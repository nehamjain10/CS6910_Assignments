# -*- coding: utf-8 -*-
"""FunctionApproximation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rZnVBEnUn0ecNvnGpJVtuHc8Z0C9MkQR
"""

"""
This is an implementation for Function Approximation using ANN in pytorch
__author__ = Lakshya J
"""

import torch
from torch import nn, optim
import torch.nn as nn
from torch.autograd import Variable
from sklearn.model_selection import train_test_split
import pandas as pd
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

class MLFFNN(nn.Module):
    """
    Class of Multi Layer Feed Forward Neural Network (MLFFNN)
    """
    def __init__(self, hidden_dim1, hidden_dim2) :
        super(MLFFNN, self).__init__()
        # adding linear and non-linear hidden layers
        self.mlffnn = nn.Sequential(nn.Linear(2, hidden_dim1),
                                       nn.Tanh(),
                                       nn.Linear(hidden_dim1, hidden_dim2),
                                       nn.Tanh(),
                                       nn.Linear(hidden_dim2, 1))
        
    def forward(self, X):
        y = self.mlffnn(X)
        return y

df = pd.read_csv('/content/func_app0.csv')
X = df.drop(columns = ['y']).copy()
y = df['y']
X = np.array(X)
y = np.array(y)

# Plot of the desired function
fig = plt.figure(figsize =(14, 9))
ax = plt.axes(projection ='3d')
# Creating plot
ax.plot_trisurf(X[:, 0], X[:, 1], y)
ax.set_xlabel('X1 axis')
ax.set_ylabel('X2 axis')
ax.set_zlabel('Y axis')
ax.set_title('Actual Function Surface')
# show plot
plt.show()

LR = [1e-5, 1e-4, 1e-3, 1e-2]
MAX_EPOCH = 2000
NEURONS = [8, 16, 32, 50, 64, 256, 512, 784]

"""Batch size = 16"""

BATCH_SIZE = 16

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.7)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.67)

X_train, y_train, X_valid, X_test, y_valid, y_test = map(torch.tensor, [X_train, y_train, X_valid, X_test, y_valid, y_test])
train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=BATCH_SIZE,
                              pin_memory=True, shuffle=True)
valid_dataloader = DataLoader(TensorDataset(X_valid.unsqueeze(1), y_valid.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)
test_dataloader = DataLoader(TensorDataset(X_test.unsqueeze(1), y_test.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)

print("BATCH SIZE =", BATCH_SIZE)
# training loop

train_loss_list = list()
val_loss_list = list()
for lr in LR:
    for neur in NEURONS:
        model = MLFFNN(neur, neur).to(device)
        optimizer = optim.SGD(model.parameters(), lr=lr)
        criterion = nn.MSELoss(reduction="mean")
        for epoch in range(MAX_EPOCH):
            model.train()
            # training loop
            for X_train, y_train in train_dataloader:
                X_train = X_train.type(torch.float32).to(device)
                y_train = y_train.type(torch.float32).to(device)
                optimizer.zero_grad()
                score = model(X_train)
                y_train = y_train.unsqueeze(1)
                loss = criterion(input=score, target=y_train)
                loss.backward()
                optimizer.step()

            # Validation
            model.eval()
            temp_loss_list = list()
            for X_valid, y_valid in valid_dataloader:
                X_valid = X_valid.type(torch.float32).to(device)
                y_valid = y_valid.type(torch.float32).to(device)

                score = model(X_valid)
                y_valid = y_valid.unsqueeze(1)
                loss = criterion(input=score, target=y_valid)

                temp_loss_list.append(loss.detach().cpu().numpy())
            
            train_loss_list.append(np.average(temp_loss_list))

        print("Learning Rate: {0}\tNo. of Neurons: {1}\tValidation loss: {2}".format(lr, neur, train_loss_list[-1]))

"""BATCH_SIZE = 1

In this case, both the hidden layers have the same number of neurons, varying the LR with number of hidden neurons
"""

BATCH_SIZE = 1

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.7)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.67)

X_train, y_train, X_valid, X_test, y_valid, y_test = map(torch.tensor, [X_train, y_train, X_valid, X_test, y_valid, y_test])
train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=BATCH_SIZE,
                              pin_memory=True, shuffle=True)
valid_dataloader = DataLoader(TensorDataset(X_valid.unsqueeze(1), y_valid.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)
test_dataloader = DataLoader(TensorDataset(X_test.unsqueeze(1), y_test.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)

print("BATCH SIZE =", BATCH_SIZE)
# training loop
train_loss_list = list()
val_loss_list = list()
for lr in LR:
    for neur in NEURONS:
        model = MLFFNN(neur, neur).to(device)
        optimizer = optim.SGD(model.parameters(), lr=lr)
        criterion = nn.MSELoss(reduction="mean")
        for epoch in range(MAX_EPOCH):
            model.train()
            # training loop
            for X_train, y_train in train_dataloader:
                X_train = X_train.type(torch.float32).to(device)
                y_train = y_train.type(torch.float32).to(device)
                optimizer.zero_grad()
                score = model(X_train)
                y_train = y_train.unsqueeze(1)
                loss = criterion(input=score, target=y_train)
                loss.backward()
                optimizer.step()

            # Validation
            model.eval()
            temp_loss_list = list()
            for X_valid, y_valid in valid_dataloader:
                X_valid = X_valid.type(torch.float32).to(device)
                y_valid = y_valid.type(torch.float32).to(device)

                score = model(X_valid)
                y_valid = y_valid.unsqueeze(1)
                loss = criterion(input=score, target=y_valid)

                temp_loss_list.append(loss.detach().cpu().numpy())
            
            train_loss_list.append(np.average(temp_loss_list))

        print("Learning Rate: {0}\tNo. of Neurons: {1}\tValidation loss: {2}".format(lr, neur, train_loss_list[-1]))

"""Checking best number of neurons in Hidden layers 1 and 2 when varying learning rate"""

BATCH_SIZE = 1
MAX_EPOCH = 2000

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.7)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.67)

X_train, y_train, X_valid, X_test, y_valid, y_test = map(torch.tensor, [X_train, y_train, X_valid, X_test, y_valid, y_test])
train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=BATCH_SIZE,
                              pin_memory=True, shuffle=True)
valid_dataloader = DataLoader(TensorDataset(X_valid.unsqueeze(1), y_valid.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)
test_dataloader = DataLoader(TensorDataset(X_test.unsqueeze(1), y_test.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)


train_loss_list = list()
val_loss_list = list()
hidden_dim_1 = [8, 16, 32, 50, 64]  
hidden_dim_2 = [8, 16, 32, 50, 64]
for lr in LR:
    for hid_dim1 in hidden_dim_1:
        for hid_dim2 in hidden_dim_2:
            model = MLFFNN(hid_dim1, hid_dim2).to(device)
            optimizer = optim.SGD(model.parameters(), lr=lr)
            criterion = nn.MSELoss(reduction="mean")
            for epoch in range(MAX_EPOCH):
                model.train()
                # training loop
                for X_train, y_train in train_dataloader:
                    X_train = X_train.type(torch.float32).to(device)
                    y_train = y_train.type(torch.float32).to(device)
                    optimizer.zero_grad()
                    score = model(X_train)
                    y_train = y_train.unsqueeze(1)
                    loss = criterion(input=score, target=y_train)
                    loss.backward()
                    optimizer.step()

                # Validation
                model.eval()
                temp_loss_list = list()
                for X_valid, y_valid in valid_dataloader:
                    X_valid = X_valid.type(torch.float32).to(device)
                    y_valid = y_valid.type(torch.float32).to(device)

                    score = model(X_valid)
                    y_valid = y_valid.unsqueeze(1)
                    loss = criterion(input=score, target=y_valid)

                    temp_loss_list.append(loss.detach().cpu().numpy())
                
                train_loss_list.append(np.average(temp_loss_list))

            print("Learning Rate: {0}\tNo. of Neurons: {1}, {2} \tValidation loss: {3}".format(lr, hid_dim1, hid_dim2, train_loss_list[-1]))

"""Checking best LR"""

BATCH_SIZE = 1
MAX_EPOCH = 2000

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.7)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.67)

X_train, y_train, X_valid, X_test, y_valid, y_test = map(torch.tensor, [X_train, y_train, X_valid, X_test, y_valid, y_test])
train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=BATCH_SIZE,
                              pin_memory=True, shuffle=True)
valid_dataloader = DataLoader(TensorDataset(X_valid.unsqueeze(1), y_valid.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)
test_dataloader = DataLoader(TensorDataset(X_test.unsqueeze(1), y_test.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)


train_loss_list = list()
val_loss_list = list()
LR = [1e-5, 1e-4, 1e-3, 1e-2]
for lr in LR:
    model = MLFFNN(32, 64).to(device)
    optimizer = optim.SGD(model.parameters(), lr=lr)
    criterion = nn.MSELoss(reduction="mean")
    for epoch in range(MAX_EPOCH):
        model.train()
        # training loop
        for X_train, y_train in train_dataloader:
            X_train = X_train.type(torch.float32).to(device)
            y_train = y_train.type(torch.float32).to(device)
            optimizer.zero_grad()
            score = model(X_train)
            y_train = y_train.unsqueeze(1)
            loss = criterion(input=score, target=y_train)
            loss.backward()
            optimizer.step()

        # Validation
        model.eval()
        temp_loss_list = list()
        for X_valid, y_valid in valid_dataloader:
            X_valid = X_valid.type(torch.float32).to(device)
            y_valid = y_valid.type(torch.float32).to(device)

            score = model(X_valid)
            y_valid = y_valid.unsqueeze(1)
            loss = criterion(input=score, target=y_valid)

            temp_loss_list.append(loss.detach().cpu().numpy())
        
        train_loss_list.append(np.average(temp_loss_list))

    print("Learning Rate: {0}\tNo. of Neurons: {1}, {2} \tValidation loss: {3}".format(lr, 32,64, train_loss_list[-1]))

BATCH_SIZE = 1
MAX_EPOCH = 2000

# In the first step we will split the data in training and remaining dataset
X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.7)
X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.67)

X_train, y_train, X_valid, X_test, y_valid, y_test = map(torch.tensor, [X_train, y_train, X_valid, X_test, y_valid, y_test])
train_dataloader = DataLoader(TensorDataset(X_train.unsqueeze(1), y_train.unsqueeze(1)), batch_size=BATCH_SIZE,
                              pin_memory=True, shuffle=True)
valid_dataloader = DataLoader(TensorDataset(X_valid.unsqueeze(1), y_valid.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)
test_dataloader = DataLoader(TensorDataset(X_test.unsqueeze(1), y_test.unsqueeze(1)), batch_size=BATCH_SIZE,
                            pin_memory=True, shuffle=True)


model = MLFFNN(32, 64).to(device)
optimizer = optim.SGD(model.parameters(), lr=0.01)
criterion = nn.MSELoss(reduction="mean")
train_loss_list = list()

for epoch in range(MAX_EPOCH):
    y_out_epoch = list()
    x_epoch = list()

    model.train() 
    # training loop
    temp_loss_list = list()
    for X_train, y_train in train_dataloader:
        X_train = X_train.type(torch.float32).to(device)
        y_train = y_train.type(torch.float32).to(device)
        optimizer.zero_grad()
        score = model(X_train)
        y_train = y_train.unsqueeze(1)
        y_out_epoch.append(score.detach().cpu().numpy().tolist()[0][0][0])
        x_epoch.append(X_train.detach().cpu().numpy().tolist()[0][0]) 
        loss = criterion(input=score, target=y_train)
        temp_loss_list.append(loss.detach().cpu().numpy())
        loss.backward()
        optimizer.step()
    
    train_loss_list.append(np.average(temp_loss_list))

    # Validation
    model.eval()
    temp_loss_list = list()
    for X_valid, y_valid in valid_dataloader:
        X_valid = X_valid.type(torch.float32).to(device)
        y_valid = y_valid.type(torch.float32).to(device)

        score = model(X_valid)
        y_valid = y_valid.unsqueeze(1)
        y_out_epoch.append(score.detach().cpu().numpy().tolist()[0][0][0])
        x_epoch.append(X_valid.detach().cpu().numpy().tolist()[0][0])
        loss = criterion(input=score, target=y_valid)

        temp_loss_list.append(loss.detach().cpu().numpy())
            
    temp_loss_list.append(np.average(temp_loss_list))

    # Plot approximate 3D surface Plot
    if epoch==0 or epoch==1 or epoch==9 or epoch==49 or epoch==499 or epoch==999 or epoch==1499 or epoch==1999:
        for X_test, y_test in test_dataloader:
            X_test = X_test.type(torch.float32).to(device)
            y_test = y_test.type(torch.float32).to(device)

            score = model(X_test)
            y_test = y_test.unsqueeze(1)
            y_out_epoch.append(score.detach().cpu().numpy().tolist()[0][0][0])
            x_epoch.append(X_test.detach().cpu().numpy().tolist()[0][0])

        x_epoch = np.asarray(x_epoch)
        y_out_epoch = np.asarray(y_out_epoch)
        # Plot of the desired function
        fig = plt.figure(figsize =(14, 9))
        ax = plt.axes(projection ='3d')
        # Creating plot
        ax.plot_trisurf(x_epoch[:, 0], x_epoch[:, 1], y_out_epoch)
        ax.set_xlabel('X1 axis')
        ax.set_ylabel('X2 axis')
        ax.set_zlabel('Y axis')
        ax.set_title('Approximate Function Surface at epoch %d' %int(epoch+1))
        # show plot
        plt.show()

    # Printing the Validation set loss every 100 epochs
    if epoch%100 == 0:
        print("\tValidation average loss at epoch {0}: {1}".format(epoch, temp_loss_list[-1]))


# Plotting the average training data error with epochs
plt.plot(range(1, MAX_EPOCH+1), train_loss_list)
plt.xlabel('Epoch')
plt.ylabel('Training data error')
plt.title('Training Data Error vs Epoch')
plt.grid()
plt.show()

# testing with the final hyperparameters - No of neurons and learning rate
model.eval()
    
temp_loss_list = list()
for X_test, y_test in test_dataloader:
    X_test = X_test.type(torch.float32).to(device)
    y_test = y_test.type(torch.float32).to(device)

    score = model(X_test)
    y_test = y_test.unsqueeze(1)
    loss = criterion(input=score, target=y_test)

    temp_loss_list.append(loss.detach().cpu().numpy())
    
temp_loss_list.append(np.average(temp_loss_list))
print("Train average loss: %.5f" %train_loss_list[-1])
print("Test average loss: %.5f" %temp_loss_list[-1])

# Scatter plot on evaluated model for training data set
train_output = list()
train_actual_output = list()
for X_train, y_train in train_dataloader:
    X_train = X_train.type(torch.float32).to(device)
    y_train = y_train.type(torch.float32).to(device)

    score = model(X_train)
    train_output.append(score.detach().cpu().numpy().tolist()[0])
    train_actual_output.append(y_train.tolist()[0])

#Scatter plot of the trained model with training data - actual result vs obtained reult
plt.scatter(train_actual_output, train_output, c ="red", linewidths = 2, marker ="s", edgecolor ="green")
plt.xlabel('Actual output of training data')
plt.ylabel('Approximated output of training data')
plt.show()

