{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AybCwCfAKPDs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('datasets0/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YJP_J-BPPZ7c"
   },
   "outputs": [],
   "source": [
    "df.drop(['UserId', 'Id', 'ProductId', 'ProfileName', 'Time', 'HelpfulnessNumerator', 'HelpfulnessDenominator'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OXcyQz_5Ll16"
   },
   "outputs": [],
   "source": [
    "df_train = df[55000:59999]\n",
    "df_valid = df[250000:251000]\n",
    "df_test = df[251001:252000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1dqQdRTwLqL7",
    "outputId": "47176ac5-20a7-4a69-cbca-8f0e7f52e1cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>1</td>\n",
       "      <td>Worst snack I ever tasted!</td>\n",
       "      <td>These are definitely the worst snack I have ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55001</th>\n",
       "      <td>2</td>\n",
       "      <td>Very Disappointing</td>\n",
       "      <td>Based on the reviews I thought I'd give the Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55002</th>\n",
       "      <td>4</td>\n",
       "      <td>Delicious, but a little salty</td>\n",
       "      <td>Like other reviewers have said, these are a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55003</th>\n",
       "      <td>5</td>\n",
       "      <td>GF snacks, \"Just the Cheese\"</td>\n",
       "      <td>Yay! I used to be able to find these at local ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55004</th>\n",
       "      <td>2</td>\n",
       "      <td>will not re-order</td>\n",
       "      <td>I was expecting something better than what arr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score                        Summary  \\\n",
       "55000      1     Worst snack I ever tasted!   \n",
       "55001      2             Very Disappointing   \n",
       "55002      4  Delicious, but a little salty   \n",
       "55003      5   GF snacks, \"Just the Cheese\"   \n",
       "55004      2              will not re-order   \n",
       "\n",
       "                                                    Text  \n",
       "55000  These are definitely the worst snack I have ev...  \n",
       "55001  Based on the reviews I thought I'd give the Wi...  \n",
       "55002  Like other reviewers have said, these are a li...  \n",
       "55003  Yay! I used to be able to find these at local ...  \n",
       "55004  I was expecting something better than what arr...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33JsnLZsNHWF",
    "outputId": "fbb6e786-4053-4542-da38-09efcbe22ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RHdDNH6b3Q7"
   },
   "source": [
    "## Tokenization and Input Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oUDNHqIvNMee"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Em3EjWbFRHVx",
    "outputId": "6b4174c8-be94-4445-9ab2-379391fada9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "Processed:  I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than most.\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0\n",
    "print('Original: ', df['Text'][0])\n",
    "print('Processed: ', text_preprocessing(df['Text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "93e47265d4994aa789abed0d50def9b3",
      "e2b6e0cd20794e22940abeeb42cc8e96",
      "599dce4954174f2b8909015d4c377290",
      "5e767bd5985044f888d84bd31230738c",
      "7959978aa50b4ebf9441f02615f1294d",
      "452f0140f7114b91adbfa09ecb3fb56c",
      "c13f3c74b4804dc89c5c89ed9b81b202",
      "8330658fd0824df29ba8e8450b787497",
      "abdfcfab69364a65a8233b0389266d01",
      "affa2a97c9c245d59365f74bce6b9772",
      "af440b9acaff473e8269ae3944d693e9",
      "8f82b402fd344929814a2dce4aacf700",
      "4ba590880552402b97ae6c43a1b2d3c5",
      "3d79ec161c8b4336b1daf0edb9235215",
      "5b9737421df24fe0bdde038e8e14e8b4",
      "1655c36f82c840278690f1070561b072",
      "e526c559a5bb4ec1ad552ada9a7a8958",
      "8c919b6325354ec4abddc7aaebb445b4",
      "3dda98c8377844138d4624931f054491",
      "7c50bf70806c478fa48df1df201ec733",
      "d1b6df08e9b24e96a67a995509a2e10d",
      "fe73813b431e4475bdb0972f16815b2a",
      "29f751fc3d35416b88a2d6a524578b63",
      "487f9e5fe119446a897a3a95ac38c65d",
      "a430ffcaba4145439221bddadc477815",
      "aed444330bf64b29aa99f43fa835473e",
      "7f163878271e48f1a758bc24d43e5978",
      "aebeca6d11564533b0e2885028ac2c32",
      "8243c1bcf44146b999b03420bb2b8e66",
      "1e510cb422654cc7af47d5fe066c5827",
      "4913a94fa16044e6b243aebcb64e6a31",
      "d7385c515c054a41be7896dbec63c823",
      "50f6cf48e9584298bbc1222cae097058"
     ]
    },
    "id": "TbJ9zNj9R4hW",
    "outputId": "29be256b-1fe9-4fc9-e196-16bd951431a6"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent[:MAX_LEN]),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "            padding='max_length',           # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmDgDDfkY7C2",
    "outputId": "d678d8b8-f0b3-4e84-9292-eda9f49997e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 384\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "# token_ids = list(preprocessing_for_bert(df['Text'])[0].squeeze().numpy())\n",
    "# print('Original: ', df['Text'][0])\n",
    "# print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(df_train['Text'])\n",
    "val_inputs, val_masks = preprocessing_for_bert(df_valid['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MXh1CweybuSa"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor((df_train['Score']-1).to_numpy())\n",
    "val_labels = torch.tensor((df_valid['Score']-1).to_numpy())\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtF5KDxtb69Q"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gqFcAVA3gfJP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 0 ns, total: 28 ms\n",
      "Wall time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 5\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "G6wtwgUCiNwp"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=2e-4,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4VheXWv9iY18"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"\n",
    "    Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            # Zero out any previously calculated gradients\n",
    "            #model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/neham/anaconda3/envs/seathru/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y5QFFGOfiqev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.014283   |     -      |     -     |   3.73   \n",
      "   1    |   40    |   1.115069   |     -      |     -     |   2.25   \n",
      "   1    |   60    |   1.616656   |     -      |     -     |   2.25   \n",
      "   1    |   80    |   1.147929   |     -      |     -     |   2.26   \n",
      "   1    |   100   |   1.194802   |     -      |     -     |   2.25   \n",
      "   1    |   120   |   1.298855   |     -      |     -     |   2.25   \n",
      "   1    |   140   |   1.307164   |     -      |     -     |   2.26   \n",
      "   1    |   160   |   1.192954   |     -      |     -     |   2.26   \n",
      "   1    |   180   |   1.169830   |     -      |     -     |   2.26   \n",
      "   1    |   200   |   1.112374   |     -      |     -     |   2.26   \n",
      "   1    |   220   |   1.166562   |     -      |     -     |   2.27   \n",
      "   1    |   240   |   0.882768   |     -      |     -     |   2.26   \n",
      "   1    |   260   |   0.869764   |     -      |     -     |   2.26   \n",
      "   1    |   280   |   1.180562   |     -      |     -     |   2.26   \n",
      "   1    |   300   |   1.319797   |     -      |     -     |   2.26   \n",
      "   1    |   320   |   0.938439   |     -      |     -     |   2.27   \n",
      "   1    |   340   |   1.619727   |     -      |     -     |   2.27   \n",
      "   1    |   360   |   1.837830   |     -      |     -     |   2.26   \n",
      "   1    |   380   |   1.000347   |     -      |     -     |   2.27   \n",
      "   1    |   400   |   0.911631   |     -      |     -     |   2.27   \n",
      "   1    |   420   |   1.434850   |     -      |     -     |   2.26   \n",
      "   1    |   440   |   1.336384   |     -      |     -     |   2.26   \n",
      "   1    |   460   |   1.144315   |     -      |     -     |   2.27   \n",
      "   1    |   480   |   1.216688   |     -      |     -     |   2.27   \n",
      "   1    |   500   |   1.154111   |     -      |     -     |   2.26   \n",
      "   1    |   520   |   0.907078   |     -      |     -     |   2.27   \n",
      "   1    |   540   |   1.205958   |     -      |     -     |   2.27   \n",
      "   1    |   560   |   1.015914   |     -      |     -     |   2.27   \n",
      "   1    |   580   |   1.318959   |     -      |     -     |   2.27   \n",
      "   1    |   600   |   1.385655   |     -      |     -     |   2.27   \n",
      "   1    |   620   |   1.523713   |     -      |     -     |   2.27   \n",
      "   1    |   640   |   1.383202   |     -      |     -     |   2.26   \n",
      "   1    |   660   |   0.789083   |     -      |     -     |   2.27   \n",
      "   1    |   680   |   0.869606   |     -      |     -     |   2.26   \n",
      "   1    |   700   |   1.334848   |     -      |     -     |   2.26   \n",
      "   1    |   720   |   1.549788   |     -      |     -     |   2.26   \n",
      "   1    |   740   |   1.229670   |     -      |     -     |   2.26   \n",
      "   1    |   760   |   1.273952   |     -      |     -     |   2.26   \n",
      "   1    |   780   |   2.207483   |     -      |     -     |   2.26   \n",
      "   1    |   800   |   1.441302   |     -      |     -     |   2.27   \n",
      "   1    |   820   |   1.141426   |     -      |     -     |   2.26   \n",
      "   1    |   840   |   0.964403   |     -      |     -     |   2.27   \n",
      "   1    |   860   |   1.151770   |     -      |     -     |   2.27   \n",
      "   1    |   880   |   0.921670   |     -      |     -     |   2.27   \n",
      "   1    |   900   |   1.240622   |     -      |     -     |   2.27   \n",
      "   1    |   920   |   1.915160   |     -      |     -     |   2.26   \n",
      "   1    |   940   |   1.308839   |     -      |     -     |   2.26   \n",
      "   1    |   960   |   1.226314   |     -      |     -     |   2.26   \n",
      "   1    |   980   |   1.446076   |     -      |     -     |   2.27   \n",
      "   1    |  1000   |   1.524212   |     -      |     -     |   2.27   \n",
      "   1    |  1020   |   1.215924   |     -      |     -     |   2.26   \n",
      "   1    |  1040   |   1.216077   |     -      |     -     |   2.27   \n",
      "   1    |  1060   |   1.026406   |     -      |     -     |   2.27   \n",
      "   1    |  1080   |   1.222086   |     -      |     -     |   2.26   \n",
      "   1    |  1100   |   1.587635   |     -      |     -     |   2.26   \n",
      "   1    |  1120   |   1.134713   |     -      |     -     |   2.26   \n",
      "   1    |  1140   |   1.485344   |     -      |     -     |   2.26   \n",
      "   1    |  1160   |   1.227749   |     -      |     -     |   2.27   \n",
      "   1    |  1180   |   0.943029   |     -      |     -     |   2.27   \n",
      "   1    |  1200   |   1.509468   |     -      |     -     |   2.27   \n",
      "   1    |  1220   |   0.847858   |     -      |     -     |   2.28   \n",
      "   1    |  1240   |   1.191377   |     -      |     -     |   2.28   \n",
      "   1    |  1260   |   1.192696   |     -      |     -     |   2.28   \n",
      "   1    |  1280   |   1.373713   |     -      |     -     |   2.28   \n",
      "   1    |  1300   |   1.291780   |     -      |     -     |   2.28   \n",
      "   1    |  1320   |   1.037307   |     -      |     -     |   2.27   \n",
      "   1    |  1340   |   1.174427   |     -      |     -     |   2.26   \n",
      "   1    |  1360   |   1.268932   |     -      |     -     |   2.27   \n",
      "   1    |  1380   |   0.905252   |     -      |     -     |   2.26   \n",
      "   1    |  1400   |   1.182122   |     -      |     -     |   2.26   \n",
      "   1    |  1420   |   0.967884   |     -      |     -     |   2.26   \n",
      "   1    |  1440   |   1.458560   |     -      |     -     |   2.26   \n",
      "   1    |  1460   |   1.335714   |     -      |     -     |   2.28   \n",
      "   1    |  1480   |   1.229202   |     -      |     -     |   2.28   \n",
      "   1    |  1500   |   1.481713   |     -      |     -     |   2.27   \n",
      "   1    |  1520   |   1.418731   |     -      |     -     |   2.28   \n",
      "   1    |  1540   |   1.069739   |     -      |     -     |   2.28   \n",
      "   1    |  1560   |   1.070873   |     -      |     -     |   2.28   \n",
      "   1    |  1580   |   1.076837   |     -      |     -     |   2.28   \n",
      "   1    |  1600   |   0.750393   |     -      |     -     |   2.28   \n",
      "   1    |  1620   |   1.417602   |     -      |     -     |   2.29   \n",
      "   1    |  1640   |   1.048193   |     -      |     -     |   2.27   \n",
      "   1    |  1660   |   1.113612   |     -      |     -     |   2.26   \n",
      "   1    |  1680   |   1.084021   |     -      |     -     |   2.26   \n",
      "   1    |  1700   |   1.025480   |     -      |     -     |   2.27   \n",
      "   1    |  1720   |   1.529415   |     -      |     -     |   2.26   \n",
      "   1    |  1740   |   1.081156   |     -      |     -     |   2.26   \n",
      "   1    |  1760   |   1.320260   |     -      |     -     |   2.27   \n",
      "   1    |  1780   |   1.323342   |     -      |     -     |   2.27   \n",
      "   1    |  1800   |   1.180051   |     -      |     -     |   2.26   \n",
      "   1    |  1820   |   1.178138   |     -      |     -     |   2.26   \n",
      "   1    |  1840   |   0.921929   |     -      |     -     |   2.28   \n",
      "   1    |  1860   |   0.967516   |     -      |     -     |   2.27   \n",
      "   1    |  1880   |   1.435895   |     -      |     -     |   2.27   \n",
      "   1    |  1900   |   1.395153   |     -      |     -     |   2.27   \n",
      "   1    |  1920   |   1.208941   |     -      |     -     |   2.28   \n",
      "   1    |  1940   |   1.308250   |     -      |     -     |   2.26   \n",
      "   1    |  1960   |   1.254779   |     -      |     -     |   2.26   \n",
      "   1    |  1980   |   1.066301   |     -      |     -     |   2.26   \n",
      "   1    |  2000   |   1.363838   |     -      |     -     |   2.27   \n",
      "   1    |  2020   |   1.304373   |     -      |     -     |   2.27   \n",
      "   1    |  2040   |   1.117402   |     -      |     -     |   2.27   \n",
      "   1    |  2060   |   1.233164   |     -      |     -     |   2.26   \n",
      "   1    |  2080   |   1.100009   |     -      |     -     |   2.27   \n",
      "   1    |  2100   |   1.446668   |     -      |     -     |   2.27   \n",
      "   1    |  2120   |   1.164144   |     -      |     -     |   2.27   \n",
      "   1    |  2140   |   1.166587   |     -      |     -     |   2.27   \n",
      "   1    |  2160   |   1.354516   |     -      |     -     |   2.27   \n",
      "   1    |  2180   |   1.152290   |     -      |     -     |   2.27   \n",
      "   1    |  2200   |   0.862790   |     -      |     -     |   2.26   \n",
      "   1    |  2220   |   1.011064   |     -      |     -     |   2.26   \n",
      "   1    |  2240   |   1.658820   |     -      |     -     |   2.27   \n",
      "   1    |  2260   |   0.889737   |     -      |     -     |   2.27   \n",
      "   1    |  2280   |   1.400186   |     -      |     -     |   2.26   \n",
      "   1    |  2300   |   1.319191   |     -      |     -     |   2.26   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  2320   |   1.324621   |     -      |     -     |   2.26   \n",
      "   1    |  2340   |   1.180793   |     -      |     -     |   2.26   \n",
      "   1    |  2360   |   0.974621   |     -      |     -     |   2.26   \n",
      "   1    |  2380   |   1.489309   |     -      |     -     |   2.26   \n",
      "   1    |  2400   |   1.472872   |     -      |     -     |   2.27   \n",
      "   1    |  2420   |   1.079488   |     -      |     -     |   2.26   \n",
      "   1    |  2440   |   1.097633   |     -      |     -     |   2.26   \n",
      "   1    |  2460   |   0.865502   |     -      |     -     |   2.26   \n",
      "   1    |  2480   |   1.215085   |     -      |     -     |   2.26   \n",
      "   1    |  2499   |   1.377729   |     -      |     -     |   2.12   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   1.222546   |  1.296027  |   60.30   |  300.72  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   1.302951   |     -      |     -     |   2.44   \n",
      "   2    |   40    |   1.438039   |     -      |     -     |   2.25   \n",
      "   2    |   60    |   1.263540   |     -      |     -     |   2.26   \n",
      "   2    |   80    |   0.904181   |     -      |     -     |   2.26   \n",
      "   2    |   100   |   1.260147   |     -      |     -     |   2.26   \n",
      "   2    |   120   |   1.179775   |     -      |     -     |   2.27   \n",
      "   2    |   140   |   1.101306   |     -      |     -     |   2.27   \n",
      "   2    |   160   |   0.845577   |     -      |     -     |   2.27   \n",
      "   2    |   180   |   1.201687   |     -      |     -     |   2.27   \n",
      "   2    |   200   |   1.238498   |     -      |     -     |   2.27   \n",
      "   2    |   220   |   1.426565   |     -      |     -     |   2.27   \n",
      "   2    |   240   |   1.000731   |     -      |     -     |   2.26   \n",
      "   2    |   260   |   1.361505   |     -      |     -     |   2.27   \n",
      "   2    |   280   |   1.280407   |     -      |     -     |   2.27   \n",
      "   2    |   300   |   1.012867   |     -      |     -     |   2.27   \n",
      "   2    |   320   |   1.708148   |     -      |     -     |   2.26   \n",
      "   2    |   340   |   1.093773   |     -      |     -     |   2.27   \n",
      "   2    |   360   |   1.234224   |     -      |     -     |   2.27   \n",
      "   2    |   380   |   0.824259   |     -      |     -     |   2.27   \n",
      "   2    |   400   |   1.460672   |     -      |     -     |   2.27   \n",
      "   2    |   420   |   1.319934   |     -      |     -     |   2.26   \n",
      "   2    |   440   |   0.764542   |     -      |     -     |   2.26   \n",
      "   2    |   460   |   1.291653   |     -      |     -     |   2.26   \n",
      "   2    |   480   |   0.958326   |     -      |     -     |   2.27   \n",
      "   2    |   500   |   1.116210   |     -      |     -     |   2.26   \n",
      "   2    |   520   |   1.502502   |     -      |     -     |   2.26   \n",
      "   2    |   540   |   1.396047   |     -      |     -     |   2.27   \n",
      "   2    |   560   |   1.009117   |     -      |     -     |   2.26   \n",
      "   2    |   580   |   1.249755   |     -      |     -     |   2.26   \n",
      "   2    |   600   |   1.369835   |     -      |     -     |   2.27   \n",
      "   2    |   620   |   1.373351   |     -      |     -     |   2.26   \n",
      "   2    |   640   |   1.253720   |     -      |     -     |   2.26   \n",
      "   2    |   660   |   0.754283   |     -      |     -     |   2.27   \n",
      "   2    |   680   |   1.203670   |     -      |     -     |   2.27   \n",
      "   2    |   700   |   1.334464   |     -      |     -     |   2.26   \n",
      "   2    |   720   |   1.162378   |     -      |     -     |   2.26   \n",
      "   2    |   740   |   1.162938   |     -      |     -     |   2.27   \n",
      "   2    |   760   |   1.158951   |     -      |     -     |   2.27   \n",
      "   2    |   780   |   1.092739   |     -      |     -     |   2.27   \n",
      "   2    |   800   |   1.083734   |     -      |     -     |   2.28   \n",
      "   2    |   820   |   1.165304   |     -      |     -     |   2.26   \n",
      "   2    |   840   |   1.660702   |     -      |     -     |   2.26   \n",
      "   2    |   860   |   1.468956   |     -      |     -     |   2.26   \n",
      "   2    |   880   |   1.018038   |     -      |     -     |   2.26   \n",
      "   2    |   900   |   0.881222   |     -      |     -     |   2.26   \n",
      "   2    |   920   |   1.400138   |     -      |     -     |   2.27   \n",
      "   2    |   940   |   1.077438   |     -      |     -     |   2.27   \n",
      "   2    |   960   |   1.077352   |     -      |     -     |   2.27   \n",
      "   2    |   980   |   1.357406   |     -      |     -     |   2.27   \n",
      "   2    |  1000   |   0.766853   |     -      |     -     |   2.26   \n",
      "   2    |  1020   |   1.522683   |     -      |     -     |   2.27   \n",
      "   2    |  1040   |   1.359306   |     -      |     -     |   2.26   \n",
      "   2    |  1060   |   1.059437   |     -      |     -     |   2.26   \n",
      "   2    |  1080   |   1.292924   |     -      |     -     |   2.27   \n",
      "   2    |  1100   |   0.897350   |     -      |     -     |   2.27   \n",
      "   2    |  1120   |   1.125112   |     -      |     -     |   2.27   \n",
      "   2    |  1140   |   1.457231   |     -      |     -     |   2.27   \n",
      "   2    |  1160   |   1.152107   |     -      |     -     |   2.26   \n",
      "   2    |  1180   |   1.517266   |     -      |     -     |   2.27   \n",
      "   2    |  1200   |   1.119749   |     -      |     -     |   2.26   \n",
      "   2    |  1220   |   1.131771   |     -      |     -     |   2.26   \n",
      "   2    |  1240   |   1.049143   |     -      |     -     |   2.27   \n",
      "   2    |  1260   |   1.379237   |     -      |     -     |   2.27   \n",
      "   2    |  1280   |   1.059186   |     -      |     -     |   2.27   \n",
      "   2    |  1300   |   1.301304   |     -      |     -     |   2.27   \n",
      "   2    |  1320   |   0.937480   |     -      |     -     |   2.26   \n",
      "   2    |  1340   |   0.791487   |     -      |     -     |   2.27   \n",
      "   2    |  1360   |   1.526063   |     -      |     -     |   2.26   \n",
      "   2    |  1380   |   1.245352   |     -      |     -     |   2.27   \n",
      "   2    |  1400   |   1.027115   |     -      |     -     |   2.27   \n",
      "   2    |  1420   |   1.049276   |     -      |     -     |   2.27   \n",
      "   2    |  1440   |   1.270517   |     -      |     -     |   2.26   \n",
      "   2    |  1460   |   1.278990   |     -      |     -     |   2.27   \n",
      "   2    |  1480   |   1.092221   |     -      |     -     |   2.27   \n",
      "   2    |  1500   |   1.053499   |     -      |     -     |   2.27   \n",
      "   2    |  1520   |   1.530364   |     -      |     -     |   2.28   \n",
      "   2    |  1540   |   0.941029   |     -      |     -     |   2.28   \n",
      "   2    |  1560   |   1.200682   |     -      |     -     |   2.28   \n",
      "   2    |  1580   |   1.134260   |     -      |     -     |   2.28   \n",
      "   2    |  1600   |   1.104733   |     -      |     -     |   2.28   \n",
      "   2    |  1620   |   1.669530   |     -      |     -     |   2.27   \n",
      "   2    |  1640   |   1.387703   |     -      |     -     |   2.28   \n",
      "   2    |  1660   |   0.965895   |     -      |     -     |   2.27   \n",
      "   2    |  1680   |   1.438246   |     -      |     -     |   2.27   \n",
      "   2    |  1700   |   1.054973   |     -      |     -     |   2.27   \n",
      "   2    |  1720   |   1.074655   |     -      |     -     |   2.26   \n",
      "   2    |  1740   |   1.155523   |     -      |     -     |   2.28   \n",
      "   2    |  1760   |   1.119335   |     -      |     -     |   2.27   \n",
      "   2    |  1780   |   0.909009   |     -      |     -     |   2.27   \n",
      "   2    |  1800   |   1.373002   |     -      |     -     |   2.28   \n",
      "   2    |  1820   |   1.319821   |     -      |     -     |   2.26   \n",
      "   2    |  1840   |   1.180847   |     -      |     -     |   2.26   \n",
      "   2    |  1860   |   0.914178   |     -      |     -     |   2.26   \n",
      "   2    |  1880   |   0.935192   |     -      |     -     |   2.26   \n",
      "   2    |  1900   |   1.652819   |     -      |     -     |   2.27   \n",
      "   2    |  1920   |   1.513564   |     -      |     -     |   2.26   \n",
      "   2    |  1940   |   1.066565   |     -      |     -     |   2.27   \n",
      "   2    |  1960   |   0.920088   |     -      |     -     |   2.27   \n",
      "   2    |  1980   |   1.133738   |     -      |     -     |   2.27   \n",
      "   2    |  2000   |   1.018581   |     -      |     -     |   2.27   \n",
      "   2    |  2020   |   0.974564   |     -      |     -     |   2.27   \n",
      "   2    |  2040   |   1.218224   |     -      |     -     |   2.27   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |  2060   |   1.423855   |     -      |     -     |   2.27   \n",
      "   2    |  2080   |   1.151823   |     -      |     -     |   2.27   \n",
      "   2    |  2100   |   1.707563   |     -      |     -     |   2.27   \n",
      "   2    |  2120   |   0.933104   |     -      |     -     |   2.26   \n",
      "   2    |  2140   |   1.620881   |     -      |     -     |   2.26   \n",
      "   2    |  2160   |   1.537265   |     -      |     -     |   2.27   \n",
      "   2    |  2180   |   0.958061   |     -      |     -     |   2.27   \n",
      "   2    |  2200   |   1.264539   |     -      |     -     |   2.26   \n",
      "   2    |  2220   |   1.240339   |     -      |     -     |   2.26   \n",
      "   2    |  2240   |   1.478502   |     -      |     -     |   2.26   \n",
      "   2    |  2260   |   1.052267   |     -      |     -     |   2.27   \n",
      "   2    |  2280   |   0.914716   |     -      |     -     |   2.26   \n",
      "   2    |  2300   |   1.468710   |     -      |     -     |   2.27   \n",
      "   2    |  2320   |   1.155161   |     -      |     -     |   2.26   \n",
      "   2    |  2340   |   1.340526   |     -      |     -     |   2.26   \n",
      "   2    |  2360   |   1.162613   |     -      |     -     |   2.28   \n",
      "   2    |  2380   |   1.134595   |     -      |     -     |   2.27   \n",
      "   2    |  2400   |   1.293788   |     -      |     -     |   2.27   \n",
      "   2    |  2420   |   1.066202   |     -      |     -     |   2.27   \n",
      "   2    |  2440   |   1.418158   |     -      |     -     |   2.26   \n",
      "   2    |  2460   |   1.170549   |     -      |     -     |   2.26   \n",
      "   2    |  2480   |   1.332754   |     -      |     -     |   2.26   \n",
      "   2    |  2499   |   1.219907   |     -      |     -     |   2.12   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   1.201787   |  1.330489  |   60.30   |  299.45  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   1.149462   |     -      |     -     |   2.45   \n",
      "   3    |   40    |   1.454277   |     -      |     -     |   2.27   \n",
      "   3    |   60    |   1.082253   |     -      |     -     |   2.26   \n",
      "   3    |   80    |   0.729805   |     -      |     -     |   2.26   \n",
      "   3    |   100   |   1.535515   |     -      |     -     |   2.26   \n",
      "   3    |   120   |   1.251962   |     -      |     -     |   2.26   \n",
      "   3    |   140   |   1.649960   |     -      |     -     |   2.26   \n",
      "   3    |   160   |   1.045841   |     -      |     -     |   2.26   \n",
      "   3    |   180   |   0.839773   |     -      |     -     |   2.27   \n",
      "   3    |   200   |   1.185458   |     -      |     -     |   2.26   \n",
      "   3    |   220   |   1.043420   |     -      |     -     |   2.26   \n",
      "   3    |   240   |   1.221490   |     -      |     -     |   2.27   \n",
      "   3    |   260   |   1.386061   |     -      |     -     |   2.27   \n",
      "   3    |   280   |   1.256142   |     -      |     -     |   2.27   \n",
      "   3    |   300   |   1.192322   |     -      |     -     |   2.27   \n",
      "   3    |   320   |   1.198309   |     -      |     -     |   2.27   \n",
      "   3    |   340   |   1.263735   |     -      |     -     |   2.27   \n",
      "   3    |   360   |   1.033552   |     -      |     -     |   2.27   \n",
      "   3    |   380   |   1.307516   |     -      |     -     |   2.26   \n",
      "   3    |   400   |   1.374323   |     -      |     -     |   2.26   \n",
      "   3    |   420   |   1.404710   |     -      |     -     |   2.27   \n",
      "   3    |   440   |   1.307472   |     -      |     -     |   2.26   \n",
      "   3    |   460   |   1.368058   |     -      |     -     |   2.26   \n",
      "   3    |   480   |   0.892269   |     -      |     -     |   2.26   \n",
      "   3    |   560   |   1.493759   |     -      |     -     |   2.27   \n",
      "   3    |   580   |   1.514459   |     -      |     -     |   2.27   \n",
      "   3    |   600   |   1.495717   |     -      |     -     |   2.27   \n",
      "   3    |   620   |   1.428995   |     -      |     -     |   2.27   \n",
      "   3    |   640   |   1.371837   |     -      |     -     |   2.27   \n",
      "   3    |   660   |   1.104790   |     -      |     -     |   2.27   \n",
      "   3    |   680   |   1.534203   |     -      |     -     |   2.27   \n",
      "   3    |   700   |   0.901498   |     -      |     -     |   2.28   \n",
      "   3    |   720   |   1.173040   |     -      |     -     |   2.28   \n",
      "   3    |   740   |   1.007557   |     -      |     -     |   2.27   \n",
      "   3    |   760   |   1.373922   |     -      |     -     |   2.27   \n",
      "   3    |   780   |   1.325000   |     -      |     -     |   2.27   \n",
      "   3    |   800   |   1.673589   |     -      |     -     |   2.27   \n",
      "   3    |   820   |   0.816459   |     -      |     -     |   2.27   \n",
      "   3    |   840   |   1.030339   |     -      |     -     |   2.27   \n",
      "   3    |   860   |   1.298153   |     -      |     -     |   2.26   \n",
      "   3    |   880   |   1.503654   |     -      |     -     |   2.27   \n",
      "   3    |   900   |   1.294264   |     -      |     -     |   2.27   \n",
      "   3    |   920   |   0.765751   |     -      |     -     |   2.27   \n",
      "   3    |   940   |   1.159215   |     -      |     -     |   2.27   \n",
      "   3    |   960   |   1.018186   |     -      |     -     |   2.26   \n",
      "   3    |   980   |   1.297261   |     -      |     -     |   2.26   \n",
      "   3    |  1000   |   0.843936   |     -      |     -     |   2.27   \n",
      "   3    |  1020   |   1.398457   |     -      |     -     |   2.27   \n",
      "   3    |  1040   |   1.265222   |     -      |     -     |   2.27   \n",
      "   3    |  1060   |   1.308175   |     -      |     -     |   2.27   \n",
      "   3    |  1080   |   1.142464   |     -      |     -     |   2.27   \n",
      "   3    |  1100   |   0.862711   |     -      |     -     |   2.28   \n",
      "   3    |  1120   |   1.128981   |     -      |     -     |   2.27   \n",
      "   3    |  1140   |   1.013266   |     -      |     -     |   2.28   \n",
      "   3    |  1160   |   0.970690   |     -      |     -     |   2.27   \n",
      "   3    |  1180   |   1.247888   |     -      |     -     |   2.27   \n",
      "   3    |  1200   |   1.255150   |     -      |     -     |   2.27   \n",
      "   3    |  1220   |   1.085451   |     -      |     -     |   2.27   \n",
      "   3    |  1240   |   1.331025   |     -      |     -     |   2.27   \n",
      "   3    |  1260   |   1.338265   |     -      |     -     |   2.27   \n",
      "   3    |  1280   |   0.910218   |     -      |     -     |   2.29   \n",
      "   3    |  1300   |   0.722712   |     -      |     -     |   2.28   \n",
      "   3    |  1320   |   1.404940   |     -      |     -     |   2.28   \n",
      "   3    |  1340   |   1.194598   |     -      |     -     |   2.27   \n",
      "   3    |  1360   |   1.195710   |     -      |     -     |   2.27   \n",
      "   3    |  1380   |   1.074598   |     -      |     -     |   2.27   \n",
      "   3    |  1400   |   0.922933   |     -      |     -     |   2.27   \n",
      "   3    |  1420   |   1.188335   |     -      |     -     |   2.27   \n",
      "   3    |  1440   |   1.189226   |     -      |     -     |   2.27   \n",
      "   3    |  1460   |   1.005161   |     -      |     -     |   2.27   \n",
      "   3    |  1480   |   1.124476   |     -      |     -     |   2.26   \n",
      "   3    |  1500   |   1.201219   |     -      |     -     |   2.27   \n",
      "   3    |  1520   |   1.343003   |     -      |     -     |   2.28   \n",
      "   3    |  1540   |   0.609075   |     -      |     -     |   2.28   \n",
      "   3    |  1560   |   1.489673   |     -      |     -     |   2.28   \n",
      "   3    |  1580   |   1.397718   |     -      |     -     |   2.27   \n",
      "   3    |  1600   |   1.501715   |     -      |     -     |   2.27   \n",
      "   3    |  1620   |   1.091838   |     -      |     -     |   2.27   \n",
      "   3    |  1640   |   1.241584   |     -      |     -     |   2.27   \n",
      "   3    |  1660   |   1.328928   |     -      |     -     |   2.27   \n",
      "   3    |  1680   |   1.223020   |     -      |     -     |   2.27   \n",
      "   3    |  1700   |   1.032343   |     -      |     -     |   2.28   \n",
      "   3    |  1720   |   1.237988   |     -      |     -     |   2.27   \n",
      "   3    |  1740   |   0.964074   |     -      |     -     |   2.26   \n",
      "   3    |  1760   |   1.313849   |     -      |     -     |   2.26   \n",
      "   3    |  1780   |   0.886552   |     -      |     -     |   2.26   \n",
      "   3    |  1800   |   1.347068   |     -      |     -     |   2.27   \n",
      "   3    |  1820   |   1.070242   |     -      |     -     |   2.27   \n",
      "   3    |  1840   |   1.215356   |     -      |     -     |   2.27   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |  1860   |   1.165858   |     -      |     -     |   2.26   \n",
      "   3    |  1880   |   0.907149   |     -      |     -     |   2.27   \n",
      "   3    |  1900   |   1.205397   |     -      |     -     |   2.27   \n",
      "   3    |  1920   |   0.843768   |     -      |     -     |   2.28   \n",
      "   3    |  1940   |   1.242648   |     -      |     -     |   2.28   \n",
      "   3    |  1960   |   0.959569   |     -      |     -     |   2.28   \n",
      "   3    |  1980   |   1.452767   |     -      |     -     |   2.28   \n",
      "   3    |  2000   |   1.073376   |     -      |     -     |   2.27   \n",
      "   3    |  2020   |   1.600255   |     -      |     -     |   2.27   \n",
      "   3    |  2040   |   1.045675   |     -      |     -     |   2.28   \n",
      "   3    |  2060   |   1.170977   |     -      |     -     |   2.28   \n",
      "   3    |  2080   |   0.919051   |     -      |     -     |   2.28   \n",
      "   3    |  2100   |   1.077822   |     -      |     -     |   2.27   \n",
      "   3    |  2120   |   1.206688   |     -      |     -     |   2.26   \n",
      "   3    |  2140   |   1.044095   |     -      |     -     |   2.27   \n",
      "   3    |  2160   |   1.216409   |     -      |     -     |   2.27   \n",
      "   3    |  2180   |   1.505982   |     -      |     -     |   2.26   \n",
      "   3    |  2200   |   1.353105   |     -      |     -     |   2.26   \n",
      "   3    |  2220   |   0.973709   |     -      |     -     |   2.26   \n",
      "   3    |  2240   |   1.427795   |     -      |     -     |   2.27   \n",
      "   3    |  2260   |   1.092654   |     -      |     -     |   2.26   \n",
      "   3    |  2280   |   1.138067   |     -      |     -     |   2.27   \n",
      "   3    |  2300   |   1.187163   |     -      |     -     |   2.26   \n",
      "   3    |  2320   |   1.327639   |     -      |     -     |   2.27   \n",
      "   3    |  2340   |   1.687744   |     -      |     -     |   2.28   \n",
      "   3    |  2360   |   1.011858   |     -      |     -     |   2.28   \n",
      "   3    |  2380   |   1.307202   |     -      |     -     |   2.27   \n",
      "   3    |  2400   |   1.365590   |     -      |     -     |   2.27   \n",
      "   3    |  2420   |   1.154066   |     -      |     -     |   2.27   \n",
      "   3    |  2440   |   1.249921   |     -      |     -     |   2.28   \n",
      "   3    |  2460   |   1.164393   |     -      |     -     |   2.27   \n",
      "   3    |  2480   |   1.229464   |     -      |     -     |   2.27   \n",
      "   3    |  2499   |   1.538517   |     -      |     -     |   2.12   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   1.204771   |  1.330489  |   60.30   |  299.75  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   1.723988   |     -      |     -     |   2.53   \n",
      "   4    |   40    |   1.490250   |     -      |     -     |   2.28   \n",
      "   4    |   60    |   1.400302   |     -      |     -     |   2.26   \n",
      "   4    |   80    |   1.132000   |     -      |     -     |   2.26   \n",
      "   4    |   100   |   1.145148   |     -      |     -     |   2.26   \n",
      "   4    |   120   |   1.334366   |     -      |     -     |   2.27   \n",
      "   4    |   140   |   1.352568   |     -      |     -     |   2.26   \n",
      "   4    |   160   |   1.007070   |     -      |     -     |   2.26   \n",
      "   4    |   180   |   0.913672   |     -      |     -     |   2.27   \n",
      "   4    |   200   |   0.878327   |     -      |     -     |   2.27   \n",
      "   4    |   220   |   1.117944   |     -      |     -     |   2.26   \n",
      "   4    |   240   |   1.286290   |     -      |     -     |   2.26   \n",
      "   4    |   260   |   0.956360   |     -      |     -     |   2.26   \n",
      "   4    |   280   |   0.828797   |     -      |     -     |   2.26   \n",
      "   4    |   300   |   0.997998   |     -      |     -     |   2.26   \n",
      "   4    |   320   |   0.903533   |     -      |     -     |   2.26   \n",
      "   4    |   340   |   1.390308   |     -      |     -     |   2.27   \n",
      "   4    |   360   |   1.152931   |     -      |     -     |   2.26   \n",
      "   4    |   380   |   1.263831   |     -      |     -     |   2.27   \n",
      "   4    |   400   |   1.096071   |     -      |     -     |   2.26   \n",
      "   4    |   420   |   1.103089   |     -      |     -     |   2.27   \n",
      "   4    |   440   |   0.724939   |     -      |     -     |   2.26   \n",
      "   4    |   460   |   1.202051   |     -      |     -     |   2.26   \n",
      "   4    |   480   |   1.089279   |     -      |     -     |   2.26   \n",
      "   4    |   500   |   0.976072   |     -      |     -     |   2.27   \n",
      "   4    |   520   |   1.062144   |     -      |     -     |   2.27   \n",
      "   4    |   540   |   0.801551   |     -      |     -     |   2.27   \n",
      "   4    |   560   |   1.249037   |     -      |     -     |   2.27   \n",
      "   4    |   580   |   1.111645   |     -      |     -     |   2.27   \n",
      "   4    |   600   |   1.619708   |     -      |     -     |   2.27   \n",
      "   4    |   620   |   1.349082   |     -      |     -     |   2.26   \n",
      "   4    |   640   |   1.218290   |     -      |     -     |   2.27   \n",
      "   4    |   660   |   1.477112   |     -      |     -     |   2.26   \n",
      "   4    |   680   |   1.334356   |     -      |     -     |   2.27   \n",
      "   4    |   700   |   1.301117   |     -      |     -     |   2.27   \n",
      "   4    |   720   |   1.339781   |     -      |     -     |   2.27   \n",
      "   4    |   740   |   1.523985   |     -      |     -     |   2.28   \n",
      "   4    |   760   |   0.980779   |     -      |     -     |   2.28   \n",
      "   4    |   780   |   1.513073   |     -      |     -     |   2.28   \n",
      "   4    |   800   |   1.188800   |     -      |     -     |   2.28   \n",
      "   4    |   820   |   1.211028   |     -      |     -     |   2.27   \n",
      "   4    |   840   |   0.972873   |     -      |     -     |   2.27   \n",
      "   4    |   860   |   0.841552   |     -      |     -     |   2.26   \n",
      "   4    |   880   |   1.388010   |     -      |     -     |   2.28   \n",
      "   4    |   900   |   1.052808   |     -      |     -     |   2.28   \n",
      "   4    |   920   |   1.544326   |     -      |     -     |   2.28   \n",
      "   4    |   940   |   1.273490   |     -      |     -     |   2.28   \n",
      "   4    |   960   |   1.182204   |     -      |     -     |   2.28   \n",
      "   4    |   980   |   0.828303   |     -      |     -     |   2.28   \n",
      "   4    |  1000   |   1.352302   |     -      |     -     |   2.28   \n",
      "   4    |  1020   |   1.438256   |     -      |     -     |   2.27   \n",
      "   4    |  1040   |   1.231139   |     -      |     -     |   2.27   \n",
      "   4    |  1060   |   0.909380   |     -      |     -     |   2.27   \n",
      "   4    |  1080   |   1.166163   |     -      |     -     |   2.26   \n",
      "   4    |  1100   |   1.084564   |     -      |     -     |   2.27   \n",
      "   4    |  1120   |   1.087086   |     -      |     -     |   2.26   \n",
      "   4    |  1140   |   1.573172   |     -      |     -     |   2.27   \n",
      "   4    |  1160   |   1.299793   |     -      |     -     |   2.27   \n",
      "   4    |  1180   |   1.527689   |     -      |     -     |   2.26   \n",
      "   4    |  1200   |   1.453110   |     -      |     -     |   2.27   \n",
      "   4    |  1220   |   1.425131   |     -      |     -     |   2.27   \n",
      "   4    |  1240   |   0.990997   |     -      |     -     |   2.28   \n",
      "   4    |  1260   |   1.284616   |     -      |     -     |   2.28   \n",
      "   4    |  1280   |   0.985868   |     -      |     -     |   2.28   \n",
      "   4    |  1300   |   1.368909   |     -      |     -     |   2.28   \n",
      "   4    |  1320   |   1.000088   |     -      |     -     |   2.27   \n",
      "   4    |  1340   |   1.006295   |     -      |     -     |   2.27   \n",
      "   4    |  1360   |   1.238192   |     -      |     -     |   2.27   \n",
      "   4    |  1380   |   1.007264   |     -      |     -     |   2.27   \n",
      "   4    |  1400   |   0.983764   |     -      |     -     |   2.26   \n",
      "   4    |  1420   |   1.084240   |     -      |     -     |   2.26   \n",
      "   4    |  1440   |   1.207314   |     -      |     -     |   2.27   \n",
      "   4    |  1460   |   1.066112   |     -      |     -     |   2.27   \n",
      "   4    |  1480   |   1.202620   |     -      |     -     |   2.27   \n",
      "   4    |  1500   |   1.295513   |     -      |     -     |   2.27   \n",
      "   4    |  1520   |   1.343837   |     -      |     -     |   2.26   \n",
      "   4    |  1540   |   0.919976   |     -      |     -     |   2.27   \n",
      "   4    |  1560   |   1.127874   |     -      |     -     |   2.28   \n",
      "   4    |  1580   |   1.345471   |     -      |     -     |   2.28   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4    |  1600   |   0.964459   |     -      |     -     |   2.28   \n",
      "   4    |  1620   |   1.603114   |     -      |     -     |   2.28   \n",
      "   4    |  1640   |   1.216750   |     -      |     -     |   2.28   \n",
      "   4    |  1660   |   1.220849   |     -      |     -     |   2.28   \n",
      "   4    |  1680   |   1.013798   |     -      |     -     |   2.28   \n",
      "   4    |  1700   |   1.267804   |     -      |     -     |   2.28   \n",
      "   4    |  1720   |   1.546869   |     -      |     -     |   2.27   \n",
      "   4    |  1740   |   1.322367   |     -      |     -     |   2.27   \n",
      "   4    |  1760   |   1.110236   |     -      |     -     |   2.26   \n",
      "   4    |  1780   |   1.125551   |     -      |     -     |   2.27   \n",
      "   4    |  1800   |   0.964874   |     -      |     -     |   2.27   \n",
      "   4    |  1820   |   1.163463   |     -      |     -     |   2.27   \n",
      "   4    |  1840   |   1.308799   |     -      |     -     |   2.26   \n",
      "   4    |  1860   |   1.246781   |     -      |     -     |   2.27   \n",
      "   4    |  1880   |   0.818785   |     -      |     -     |   2.26   \n",
      "   4    |  1900   |   1.428182   |     -      |     -     |   2.27   \n",
      "   4    |  1920   |   1.201700   |     -      |     -     |   2.27   \n",
      "   4    |  1940   |   1.235998   |     -      |     -     |   2.27   \n",
      "   4    |  1960   |   1.344371   |     -      |     -     |   2.27   \n",
      "   4    |  1980   |   1.267956   |     -      |     -     |   2.27   \n",
      "   4    |  2000   |   1.206180   |     -      |     -     |   2.26   \n",
      "   4    |  2020   |   1.395681   |     -      |     -     |   2.26   \n",
      "   4    |  2040   |   1.593704   |     -      |     -     |   2.26   \n",
      "   4    |  2060   |   1.229962   |     -      |     -     |   2.27   \n",
      "   4    |  2080   |   1.173400   |     -      |     -     |   2.27   \n",
      "   4    |  2100   |   1.040912   |     -      |     -     |   2.26   \n",
      "   4    |  2120   |   1.115290   |     -      |     -     |   2.26   \n",
      "   4    |  2140   |   1.258848   |     -      |     -     |   2.27   \n",
      "   4    |  2160   |   1.334579   |     -      |     -     |   2.27   \n",
      "   4    |  2180   |   1.189069   |     -      |     -     |   2.27   \n",
      "   4    |  2200   |   1.265910   |     -      |     -     |   2.27   \n",
      "   4    |  2220   |   1.221671   |     -      |     -     |   2.27   \n",
      "   4    |  2240   |   1.502964   |     -      |     -     |   2.27   \n",
      "   4    |  2260   |   1.127345   |     -      |     -     |   2.27   \n",
      "   4    |  2280   |   1.447997   |     -      |     -     |   2.27   \n",
      "   4    |  2300   |   1.216077   |     -      |     -     |   2.26   \n",
      "   4    |  2320   |   1.389998   |     -      |     -     |   2.27   \n",
      "   4    |  2340   |   1.454275   |     -      |     -     |   2.27   \n",
      "   4    |  2360   |   1.234877   |     -      |     -     |   2.27   \n",
      "   4    |  2380   |   1.277260   |     -      |     -     |   2.26   \n",
      "   4    |  2400   |   1.155619   |     -      |     -     |   2.27   \n",
      "   4    |  2420   |   1.204406   |     -      |     -     |   2.27   \n",
      "   4    |  2440   |   1.194090   |     -      |     -     |   2.26   \n",
      "   4    |  2460   |   0.963885   |     -      |     -     |   2.27   \n",
      "   4    |  2480   |   1.256621   |     -      |     -     |   2.27   \n",
      "   4    |  2499   |   0.889049   |     -      |     -     |   2.12   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   1.204745   |  1.330489  |   60.30   |  299.69  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   20    |   1.437441   |     -      |     -     |   2.45   \n",
      "   5    |   40    |   1.160252   |     -      |     -     |   2.26   \n",
      "   5    |   60    |   1.310414   |     -      |     -     |   2.26   \n",
      "   5    |   80    |   1.505261   |     -      |     -     |   2.26   \n",
      "   5    |   100   |   1.418867   |     -      |     -     |   2.26   \n",
      "   5    |   120   |   1.033287   |     -      |     -     |   2.26   \n",
      "   5    |   140   |   0.942623   |     -      |     -     |   2.26   \n",
      "   5    |   160   |   1.576512   |     -      |     -     |   2.27   \n",
      "   5    |   180   |   1.434519   |     -      |     -     |   2.27   \n",
      "   5    |   200   |   1.473630   |     -      |     -     |   2.27   \n",
      "   5    |   220   |   1.328535   |     -      |     -     |   2.27   \n",
      "   5    |   240   |   1.309460   |     -      |     -     |   2.27   \n",
      "   5    |   260   |   1.144694   |     -      |     -     |   2.26   \n",
      "   5    |   280   |   0.973202   |     -      |     -     |   2.26   \n",
      "   5    |   300   |   1.032189   |     -      |     -     |   2.26   \n",
      "   5    |   320   |   1.443613   |     -      |     -     |   2.26   \n",
      "   5    |   340   |   0.836348   |     -      |     -     |   2.27   \n",
      "   5    |   360   |   1.227774   |     -      |     -     |   2.26   \n",
      "   5    |   380   |   1.274500   |     -      |     -     |   2.26   \n",
      "   5    |   400   |   1.120473   |     -      |     -     |   2.26   \n",
      "   5    |   420   |   1.466688   |     -      |     -     |   2.27   \n",
      "   5    |   440   |   1.305507   |     -      |     -     |   2.26   \n",
      "   5    |   460   |   1.103571   |     -      |     -     |   2.27   \n",
      "   5    |   480   |   1.275181   |     -      |     -     |   2.27   \n",
      "   5    |   500   |   1.003408   |     -      |     -     |   2.27   \n",
      "   5    |   520   |   1.209800   |     -      |     -     |   2.26   \n",
      "   5    |   540   |   1.490402   |     -      |     -     |   2.26   \n",
      "   5    |   560   |   1.366222   |     -      |     -     |   2.27   \n",
      "   5    |   580   |   0.987851   |     -      |     -     |   2.26   \n",
      "   5    |   600   |   1.441694   |     -      |     -     |   2.27   \n",
      "   5    |   620   |   1.005032   |     -      |     -     |   2.27   \n",
      "   5    |   640   |   1.364914   |     -      |     -     |   2.27   \n",
      "   5    |   660   |   1.267714   |     -      |     -     |   2.26   \n",
      "   5    |   680   |   1.053390   |     -      |     -     |   2.27   \n",
      "   5    |   700   |   1.145479   |     -      |     -     |   2.26   \n",
      "   5    |   720   |   1.281744   |     -      |     -     |   2.27   \n",
      "   5    |   740   |   1.063817   |     -      |     -     |   2.26   \n",
      "   5    |   760   |   1.209726   |     -      |     -     |   2.26   \n",
      "   5    |   780   |   1.239176   |     -      |     -     |   2.28   \n",
      "   5    |   800   |   1.377942   |     -      |     -     |   2.27   \n",
      "   5    |   820   |   1.054710   |     -      |     -     |   2.27   \n",
      "   5    |   840   |   1.197357   |     -      |     -     |   2.27   \n",
      "   5    |   860   |   1.300172   |     -      |     -     |   2.28   \n",
      "   5    |   880   |   1.455035   |     -      |     -     |   2.28   \n",
      "   5    |   900   |   1.233541   |     -      |     -     |   2.28   \n",
      "   5    |   920   |   0.702429   |     -      |     -     |   2.28   \n",
      "   5    |   940   |   0.973569   |     -      |     -     |   2.28   \n",
      "   5    |   960   |   1.062444   |     -      |     -     |   2.28   \n",
      "   5    |   980   |   1.187939   |     -      |     -     |   2.28   \n",
      "   5    |  1000   |   1.204727   |     -      |     -     |   2.27   \n",
      "   5    |  1020   |   1.433282   |     -      |     -     |   2.27   \n",
      "   5    |  1040   |   1.580877   |     -      |     -     |   2.27   \n",
      "   5    |  1060   |   0.906560   |     -      |     -     |   2.26   \n",
      "   5    |  1080   |   1.064124   |     -      |     -     |   2.27   \n",
      "   5    |  1100   |   0.918681   |     -      |     -     |   2.27   \n",
      "   5    |  1120   |   1.405191   |     -      |     -     |   2.27   \n",
      "   5    |  1140   |   1.402277   |     -      |     -     |   2.26   \n",
      "   5    |  1160   |   0.941275   |     -      |     -     |   2.26   \n",
      "   5    |  1180   |   1.436780   |     -      |     -     |   2.26   \n",
      "   5    |  1200   |   0.670481   |     -      |     -     |   2.26   \n",
      "   5    |  1220   |   1.547390   |     -      |     -     |   2.27   \n",
      "   5    |  1240   |   0.809100   |     -      |     -     |   2.27   \n",
      "   5    |  1260   |   1.194978   |     -      |     -     |   2.26   \n",
      "   5    |  1280   |   1.758599   |     -      |     -     |   2.26   \n",
      "   5    |  1300   |   0.997410   |     -      |     -     |   2.26   \n",
      "   5    |  1320   |   1.522813   |     -      |     -     |   2.27   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5    |  1340   |   1.135156   |     -      |     -     |   2.28   \n",
      "   5    |  1360   |   1.556576   |     -      |     -     |   2.28   \n",
      "   5    |  1380   |   1.107854   |     -      |     -     |   2.28   \n",
      "   5    |  1400   |   1.187842   |     -      |     -     |   2.27   \n",
      "   5    |  1420   |   1.161608   |     -      |     -     |   2.28   \n",
      "   5    |  1440   |   1.154745   |     -      |     -     |   2.27   \n",
      "   5    |  1460   |   1.062741   |     -      |     -     |   2.27   \n",
      "   5    |  1480   |   1.215607   |     -      |     -     |   2.27   \n",
      "   5    |  1500   |   1.556597   |     -      |     -     |   2.27   \n",
      "   5    |  1520   |   1.290891   |     -      |     -     |   2.27   \n",
      "   5    |  1540   |   0.942098   |     -      |     -     |   2.28   \n",
      "   5    |  1560   |   1.261412   |     -      |     -     |   2.27   \n",
      "   5    |  1580   |   1.267443   |     -      |     -     |   2.27   \n",
      "   5    |  1600   |   1.080417   |     -      |     -     |   2.27   \n",
      "   5    |  1620   |   1.087630   |     -      |     -     |   2.27   \n",
      "   5    |  1640   |   1.113082   |     -      |     -     |   2.27   \n",
      "   5    |  1660   |   1.345564   |     -      |     -     |   2.27   \n",
      "   5    |  1680   |   1.216612   |     -      |     -     |   2.27   \n",
      "   5    |  1700   |   0.679216   |     -      |     -     |   2.27   \n",
      "   5    |  1720   |   1.160280   |     -      |     -     |   2.27   \n",
      "   5    |  1740   |   1.230490   |     -      |     -     |   2.27   \n",
      "   5    |  1760   |   1.083726   |     -      |     -     |   2.27   \n",
      "   5    |  1780   |   1.076818   |     -      |     -     |   2.26   \n",
      "   5    |  1800   |   1.231131   |     -      |     -     |   2.27   \n",
      "   5    |  1820   |   1.353500   |     -      |     -     |   2.26   \n",
      "   5    |  1840   |   1.415420   |     -      |     -     |   2.27   \n",
      "   5    |  1860   |   0.991035   |     -      |     -     |   2.26   \n",
      "   5    |  1880   |   0.997478   |     -      |     -     |   2.27   \n",
      "   5    |  1900   |   1.156111   |     -      |     -     |   2.28   \n",
      "   5    |  1920   |   0.803754   |     -      |     -     |   2.28   \n",
      "   5    |  1940   |   1.031346   |     -      |     -     |   2.27   \n",
      "   5    |  1960   |   1.481373   |     -      |     -     |   2.28   \n",
      "   5    |  1980   |   1.382774   |     -      |     -     |   2.28   \n",
      "   5    |  2000   |   1.363518   |     -      |     -     |   2.28   \n",
      "   5    |  2020   |   1.197426   |     -      |     -     |   2.28   \n",
      "   5    |  2040   |   1.633178   |     -      |     -     |   2.28   \n",
      "   5    |  2060   |   1.195762   |     -      |     -     |   2.27   \n",
      "   5    |  2080   |   1.516003   |     -      |     -     |   2.27   \n",
      "   5    |  2100   |   1.147383   |     -      |     -     |   2.27   \n",
      "   5    |  2120   |   1.196698   |     -      |     -     |   2.27   \n",
      "   5    |  2140   |   1.280394   |     -      |     -     |   2.27   \n",
      "   5    |  2160   |   1.444461   |     -      |     -     |   2.27   \n",
      "   5    |  2180   |   0.948730   |     -      |     -     |   2.27   \n",
      "   5    |  2200   |   1.551342   |     -      |     -     |   2.27   \n",
      "   5    |  2220   |   0.981803   |     -      |     -     |   2.27   \n",
      "   5    |  2240   |   1.148112   |     -      |     -     |   2.27   \n",
      "   5    |  2260   |   1.005511   |     -      |     -     |   2.27   \n",
      "   5    |  2280   |   1.112688   |     -      |     -     |   2.26   \n",
      "   5    |  2300   |   1.146077   |     -      |     -     |   2.26   \n",
      "   5    |  2320   |   1.153848   |     -      |     -     |   2.27   \n",
      "   5    |  2340   |   0.901426   |     -      |     -     |   2.27   \n",
      "   5    |  2360   |   1.331899   |     -      |     -     |   2.27   \n",
      "   5    |  2380   |   1.043756   |     -      |     -     |   2.26   \n",
      "   5    |  2400   |   0.791833   |     -      |     -     |   2.27   \n",
      "   5    |  2420   |   1.317830   |     -      |     -     |   2.26   \n",
      "   5    |  2440   |   1.368103   |     -      |     -     |   2.27   \n",
      "   5    |  2460   |   1.199386   |     -      |     -     |   2.27   \n",
      "   5    |  2480   |   1.109933   |     -      |     -     |   2.27   \n",
      "   5    |  2499   |   1.081032   |     -      |     -     |   2.12   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   1.204871   |  1.330489  |   60.30   |  299.64  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   6    |   20    |   1.390723   |     -      |     -     |   2.42   \n",
      "   6    |   40    |   0.808218   |     -      |     -     |   2.26   \n",
      "   6    |   60    |   1.100611   |     -      |     -     |   2.27   \n",
      "   6    |   80    |   1.311950   |     -      |     -     |   2.26   \n",
      "   6    |   100   |   1.421530   |     -      |     -     |   2.27   \n",
      "   6    |   120   |   1.094644   |     -      |     -     |   2.27   \n",
      "   6    |   140   |   1.694334   |     -      |     -     |   2.27   \n",
      "   6    |   160   |   1.511503   |     -      |     -     |   2.27   \n",
      "   6    |   180   |   1.247391   |     -      |     -     |   2.27   \n",
      "   6    |   200   |   0.994964   |     -      |     -     |   2.27   \n",
      "   6    |   220   |   0.908880   |     -      |     -     |   2.27   \n",
      "   6    |   240   |   1.426514   |     -      |     -     |   2.26   \n",
      "   6    |   260   |   1.308696   |     -      |     -     |   2.26   \n",
      "   6    |   280   |   1.700077   |     -      |     -     |   2.27   \n",
      "   6    |   300   |   1.528005   |     -      |     -     |   2.27   \n",
      "   6    |   320   |   1.213368   |     -      |     -     |   2.27   \n",
      "   6    |   340   |   1.255257   |     -      |     -     |   2.26   \n",
      "   6    |   360   |   1.241499   |     -      |     -     |   2.27   \n",
      "   6    |   380   |   1.059661   |     -      |     -     |   2.27   \n",
      "   6    |   400   |   1.082453   |     -      |     -     |   2.27   \n",
      "   6    |   420   |   1.119457   |     -      |     -     |   2.27   \n",
      "   6    |   440   |   0.910271   |     -      |     -     |   2.28   \n",
      "   6    |   460   |   1.545812   |     -      |     -     |   2.28   \n",
      "   6    |   480   |   1.187794   |     -      |     -     |   2.28   \n",
      "   6    |   500   |   0.945882   |     -      |     -     |   2.27   \n",
      "   6    |   520   |   0.914982   |     -      |     -     |   2.27   \n",
      "   6    |   540   |   0.918290   |     -      |     -     |   2.28   \n",
      "   6    |   560   |   1.147967   |     -      |     -     |   2.27   \n",
      "   6    |   580   |   1.213543   |     -      |     -     |   2.28   \n",
      "   6    |   600   |   1.445269   |     -      |     -     |   2.27   \n",
      "   6    |   620   |   1.313782   |     -      |     -     |   2.27   \n",
      "   6    |   640   |   1.417522   |     -      |     -     |   2.28   \n",
      "   6    |   660   |   1.281609   |     -      |     -     |   2.27   \n",
      "   6    |   680   |   0.955135   |     -      |     -     |   2.27   \n",
      "   6    |   700   |   1.213699   |     -      |     -     |   2.27   \n",
      "   6    |   720   |   1.204427   |     -      |     -     |   2.27   \n",
      "   6    |   740   |   0.945219   |     -      |     -     |   2.27   \n",
      "   6    |   760   |   1.406944   |     -      |     -     |   2.26   \n",
      "   6    |   780   |   1.013097   |     -      |     -     |   2.26   \n",
      "   6    |   800   |   1.021898   |     -      |     -     |   2.26   \n",
      "   6    |   820   |   1.289500   |     -      |     -     |   2.27   \n",
      "   6    |   840   |   0.972590   |     -      |     -     |   2.27   \n",
      "   6    |   860   |   1.151325   |     -      |     -     |   2.27   \n",
      "   6    |   880   |   1.632750   |     -      |     -     |   2.26   \n",
      "   6    |   900   |   1.111294   |     -      |     -     |   2.26   \n",
      "   6    |   920   |   0.956427   |     -      |     -     |   2.26   \n",
      "   6    |   940   |   1.013707   |     -      |     -     |   2.26   \n",
      "   6    |   960   |   0.952763   |     -      |     -     |   2.26   \n",
      "   6    |   980   |   1.401154   |     -      |     -     |   2.27   \n",
      "   6    |  1000   |   0.729964   |     -      |     -     |   2.27   \n",
      "   6    |  1020   |   0.864275   |     -      |     -     |   2.26   \n",
      "   6    |  1040   |   1.473954   |     -      |     -     |   2.26   \n",
      "   6    |  1060   |   1.215552   |     -      |     -     |   2.26   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6    |  1080   |   1.416867   |     -      |     -     |   2.27   \n",
      "   6    |  1100   |   1.017336   |     -      |     -     |   2.27   \n",
      "   6    |  1120   |   1.167601   |     -      |     -     |   2.27   \n",
      "   6    |  1140   |   1.343446   |     -      |     -     |   2.27   \n",
      "   6    |  1160   |   0.884829   |     -      |     -     |   2.27   \n",
      "   6    |  1180   |   1.333849   |     -      |     -     |   2.26   \n",
      "   6    |  1200   |   0.973333   |     -      |     -     |   2.26   \n",
      "   6    |  1220   |   1.372016   |     -      |     -     |   2.27   \n",
      "   6    |  1240   |   1.276063   |     -      |     -     |   2.28   \n",
      "   6    |  1260   |   1.491168   |     -      |     -     |   2.27   \n",
      "   6    |  1280   |   1.199625   |     -      |     -     |   2.27   \n",
      "   6    |  1300   |   0.802887   |     -      |     -     |   2.29   \n",
      "   6    |  1320   |   0.756282   |     -      |     -     |   2.28   \n",
      "   6    |  1340   |   1.179131   |     -      |     -     |   2.27   \n",
      "   6    |  1360   |   1.168641   |     -      |     -     |   2.27   \n",
      "   6    |  1380   |   0.809559   |     -      |     -     |   2.27   \n",
      "   6    |  1400   |   0.956591   |     -      |     -     |   2.26   \n",
      "   6    |  1420   |   1.151675   |     -      |     -     |   2.27   \n",
      "   6    |  1440   |   1.272694   |     -      |     -     |   2.27   \n",
      "   6    |  1460   |   1.080857   |     -      |     -     |   2.27   \n",
      "   6    |  1480   |   1.212462   |     -      |     -     |   2.27   \n",
      "   6    |  1500   |   1.207073   |     -      |     -     |   2.26   \n",
      "   6    |  1520   |   1.030983   |     -      |     -     |   2.27   \n",
      "   6    |  1540   |   1.326116   |     -      |     -     |   2.28   \n",
      "   6    |  1560   |   1.517147   |     -      |     -     |   2.28   \n",
      "   6    |  1580   |   1.249995   |     -      |     -     |   2.27   \n",
      "   6    |  1600   |   1.261800   |     -      |     -     |   2.27   \n",
      "   6    |  1620   |   1.546956   |     -      |     -     |   2.27   \n",
      "   6    |  1640   |   1.135361   |     -      |     -     |   2.27   \n",
      "   6    |  1660   |   1.221243   |     -      |     -     |   2.27   \n",
      "   6    |  1680   |   1.382846   |     -      |     -     |   2.27   \n",
      "   6    |  1700   |   1.086318   |     -      |     -     |   2.26   \n",
      "   6    |  1720   |   1.204941   |     -      |     -     |   2.27   \n",
      "   6    |  1740   |   1.496356   |     -      |     -     |   2.28   \n",
      "   6    |  1760   |   0.952860   |     -      |     -     |   2.27   \n",
      "   6    |  1780   |   1.344277   |     -      |     -     |   2.27   \n",
      "   6    |  1800   |   1.438765   |     -      |     -     |   2.26   \n",
      "   6    |  1820   |   1.436197   |     -      |     -     |   2.26   \n",
      "   6    |  1840   |   1.159629   |     -      |     -     |   2.27   \n",
      "   6    |  1860   |   1.206200   |     -      |     -     |   2.27   \n",
      "   6    |  1880   |   0.969307   |     -      |     -     |   2.27   \n",
      "   6    |  1900   |   0.975553   |     -      |     -     |   2.26   \n",
      "   6    |  1920   |   1.560706   |     -      |     -     |   2.27   \n",
      "   6    |  1940   |   0.855733   |     -      |     -     |   2.27   \n",
      "   6    |  1960   |   1.276086   |     -      |     -     |   2.27   \n",
      "   6    |  1980   |   1.308520   |     -      |     -     |   2.27   \n",
      "   6    |  2000   |   1.401183   |     -      |     -     |   2.27   \n",
      "   6    |  2020   |   1.605665   |     -      |     -     |   2.27   \n",
      "   6    |  2040   |   1.121812   |     -      |     -     |   2.27   \n",
      "   6    |  2060   |   1.156200   |     -      |     -     |   2.27   \n",
      "   6    |  2080   |   1.468092   |     -      |     -     |   2.27   \n",
      "   6    |  2100   |   1.512577   |     -      |     -     |   2.26   \n",
      "   6    |  2120   |   1.157706   |     -      |     -     |   2.27   \n",
      "   6    |  2140   |   1.171302   |     -      |     -     |   2.26   \n",
      "   6    |  2160   |   1.264259   |     -      |     -     |   2.26   \n",
      "   6    |  2180   |   1.025242   |     -      |     -     |   2.27   \n",
      "   6    |  2200   |   1.385442   |     -      |     -     |   2.27   \n",
      "   6    |  2220   |   1.423448   |     -      |     -     |   2.27   \n",
      "   6    |  2240   |   1.090082   |     -      |     -     |   2.27   \n",
      "   6    |  2260   |   1.455893   |     -      |     -     |   2.27   \n",
      "   6    |  2280   |   0.834211   |     -      |     -     |   2.27   \n",
      "   6    |  2300   |   1.522125   |     -      |     -     |   2.28   \n",
      "   6    |  2320   |   0.759187   |     -      |     -     |   2.28   \n",
      "   6    |  2340   |   1.353109   |     -      |     -     |   2.27   \n",
      "   6    |  2360   |   1.415271   |     -      |     -     |   2.27   \n",
      "   6    |  2380   |   1.047425   |     -      |     -     |   2.27   \n",
      "   6    |  2400   |   1.456415   |     -      |     -     |   2.27   \n",
      "   6    |  2420   |   1.195365   |     -      |     -     |   2.27   \n",
      "   6    |  2440   |   1.118075   |     -      |     -     |   2.27   \n",
      "   6    |  2460   |   1.688314   |     -      |     -     |   2.27   \n",
      "   6    |  2480   |   1.052943   |     -      |     -     |   2.26   \n",
      "   6    |  2499   |   0.619850   |     -      |     -     |   2.12   \n",
      "----------------------------------------------------------------------\n",
      "   6    |    -    |   1.204366   |  1.330489  |   60.30   |  299.66  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   7    |   20    |   1.508451   |     -      |     -     |   2.45   \n",
      "   7    |   40    |   1.366444   |     -      |     -     |   2.26   \n",
      "   7    |   60    |   1.083239   |     -      |     -     |   2.27   \n",
      "   7    |   80    |   1.459709   |     -      |     -     |   2.26   \n",
      "   7    |   100   |   1.595753   |     -      |     -     |   2.26   \n",
      "   7    |   120   |   1.353169   |     -      |     -     |   2.26   \n",
      "   7    |   140   |   1.261402   |     -      |     -     |   2.26   \n",
      "   7    |   160   |   1.537317   |     -      |     -     |   2.27   \n",
      "   7    |   180   |   1.435475   |     -      |     -     |   2.27   \n",
      "   7    |   200   |   0.661563   |     -      |     -     |   2.26   \n",
      "   7    |   220   |   1.114495   |     -      |     -     |   2.27   \n",
      "   7    |   240   |   1.041644   |     -      |     -     |   2.27   \n",
      "   7    |   260   |   1.198762   |     -      |     -     |   2.27   \n",
      "   7    |   280   |   1.333985   |     -      |     -     |   2.26   \n",
      "   7    |   300   |   0.958654   |     -      |     -     |   2.27   \n",
      "   7    |   320   |   1.192952   |     -      |     -     |   2.26   \n",
      "   7    |   340   |   1.361583   |     -      |     -     |   2.26   \n",
      "   7    |   360   |   0.894484   |     -      |     -     |   2.26   \n",
      "   7    |   380   |   1.491330   |     -      |     -     |   2.26   \n",
      "   7    |   400   |   0.944480   |     -      |     -     |   2.27   \n",
      "   7    |   420   |   1.009477   |     -      |     -     |   2.27   \n",
      "   7    |   440   |   1.062146   |     -      |     -     |   2.27   \n",
      "   7    |   460   |   1.070862   |     -      |     -     |   2.26   \n",
      "   7    |   480   |   1.036456   |     -      |     -     |   2.27   \n",
      "   7    |   500   |   1.629172   |     -      |     -     |   2.26   \n",
      "   7    |   520   |   0.790758   |     -      |     -     |   2.26   \n",
      "   7    |   540   |   1.156242   |     -      |     -     |   2.27   \n",
      "   7    |   560   |   0.784156   |     -      |     -     |   2.27   \n",
      "   7    |   580   |   1.326397   |     -      |     -     |   2.27   \n",
      "   7    |   600   |   1.008594   |     -      |     -     |   2.26   \n",
      "   7    |   620   |   1.300049   |     -      |     -     |   2.27   \n",
      "   7    |   640   |   1.476144   |     -      |     -     |   2.27   \n",
      "   7    |   660   |   1.148719   |     -      |     -     |   2.27   \n",
      "   7    |   680   |   1.420077   |     -      |     -     |   2.26   \n",
      "   7    |   700   |   1.535102   |     -      |     -     |   2.26   \n",
      "   7    |   720   |   1.128672   |     -      |     -     |   2.26   \n",
      "   7    |   740   |   1.193617   |     -      |     -     |   2.27   \n",
      "   7    |   760   |   1.348685   |     -      |     -     |   2.27   \n",
      "   7    |   780   |   1.204761   |     -      |     -     |   2.27   \n",
      "   7    |   800   |   1.389243   |     -      |     -     |   2.27   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7    |   820   |   1.152052   |     -      |     -     |   2.26   \n",
      "   7    |   840   |   1.032251   |     -      |     -     |   2.27   \n",
      "   7    |   860   |   1.088742   |     -      |     -     |   2.27   \n",
      "   7    |   880   |   1.397903   |     -      |     -     |   2.28   \n",
      "   7    |   900   |   1.272703   |     -      |     -     |   2.28   \n",
      "   7    |   920   |   1.238581   |     -      |     -     |   2.28   \n",
      "   7    |   940   |   1.113558   |     -      |     -     |   2.27   \n",
      "   7    |   960   |   1.663264   |     -      |     -     |   2.28   \n",
      "   7    |   980   |   1.200361   |     -      |     -     |   2.28   \n",
      "   7    |  1000   |   1.386536   |     -      |     -     |   2.27   \n",
      "   7    |  1020   |   1.554065   |     -      |     -     |   2.27   \n",
      "   7    |  1040   |   1.194261   |     -      |     -     |   2.27   \n",
      "   7    |  1060   |   0.985493   |     -      |     -     |   2.26   \n",
      "   7    |  1080   |   0.880409   |     -      |     -     |   2.26   \n",
      "   7    |  1100   |   1.108710   |     -      |     -     |   2.26   \n",
      "   7    |  1120   |   1.207463   |     -      |     -     |   2.26   \n",
      "   7    |  1140   |   0.944271   |     -      |     -     |   2.26   \n",
      "   7    |  1160   |   1.263515   |     -      |     -     |   2.26   \n",
      "   7    |  1180   |   1.378333   |     -      |     -     |   2.27   \n",
      "   7    |  1200   |   1.289611   |     -      |     -     |   2.26   \n",
      "   7    |  1220   |   1.350224   |     -      |     -     |   2.27   \n",
      "   7    |  1240   |   1.158901   |     -      |     -     |   2.27   \n",
      "   7    |  1260   |   1.740621   |     -      |     -     |   2.27   \n",
      "   7    |  1280   |   1.006632   |     -      |     -     |   2.27   \n",
      "   7    |  1300   |   1.289916   |     -      |     -     |   2.27   \n",
      "   7    |  1320   |   1.204277   |     -      |     -     |   2.27   \n",
      "   7    |  1340   |   1.414488   |     -      |     -     |   2.27   \n",
      "   7    |  1360   |   0.944248   |     -      |     -     |   2.27   \n",
      "   7    |  1380   |   1.274181   |     -      |     -     |   2.27   \n",
      "   7    |  1400   |   1.229734   |     -      |     -     |   2.27   \n",
      "   7    |  1420   |   1.206369   |     -      |     -     |   2.27   \n",
      "   7    |  1440   |   0.924739   |     -      |     -     |   2.27   \n",
      "   7    |  1460   |   1.103143   |     -      |     -     |   2.27   \n",
      "   7    |  1480   |   1.129401   |     -      |     -     |   2.27   \n",
      "   7    |  1500   |   1.498738   |     -      |     -     |   2.27   \n",
      "   7    |  1520   |   1.005590   |     -      |     -     |   2.27   \n",
      "   7    |  1540   |   0.835300   |     -      |     -     |   2.26   \n",
      "   7    |  1560   |   0.964126   |     -      |     -     |   2.27   \n",
      "   7    |  1580   |   0.768893   |     -      |     -     |   2.27   \n",
      "   7    |  1600   |   0.837401   |     -      |     -     |   2.27   \n",
      "   7    |  1620   |   1.452004   |     -      |     -     |   2.26   \n",
      "   7    |  1640   |   1.279844   |     -      |     -     |   2.27   \n",
      "   7    |  1660   |   1.221224   |     -      |     -     |   2.27   \n",
      "   7    |  1680   |   1.233997   |     -      |     -     |   2.27   \n",
      "   7    |  1700   |   1.452035   |     -      |     -     |   2.27   \n",
      "   7    |  1720   |   1.173628   |     -      |     -     |   2.27   \n",
      "   7    |  1740   |   1.044250   |     -      |     -     |   2.27   \n",
      "   7    |  1760   |   1.472628   |     -      |     -     |   2.27   \n",
      "   7    |  1780   |   1.295751   |     -      |     -     |   2.27   \n",
      "   7    |  1800   |   1.189786   |     -      |     -     |   2.27   \n",
      "   7    |  1820   |   1.168367   |     -      |     -     |   2.27   \n",
      "   7    |  1840   |   0.816744   |     -      |     -     |   2.27   \n",
      "   7    |  1860   |   1.460909   |     -      |     -     |   2.27   \n",
      "   7    |  1880   |   0.982897   |     -      |     -     |   2.27   \n",
      "   7    |  1900   |   1.116480   |     -      |     -     |   2.27   \n",
      "   7    |  1920   |   1.097571   |     -      |     -     |   2.27   \n",
      "   7    |  1940   |   1.156502   |     -      |     -     |   2.27   \n",
      "   7    |  1960   |   1.260274   |     -      |     -     |   2.27   \n",
      "   7    |  1980   |   1.403288   |     -      |     -     |   2.27   \n",
      "   7    |  2000   |   1.093058   |     -      |     -     |   2.27   \n",
      "   7    |  2020   |   1.136229   |     -      |     -     |   2.27   \n",
      "   7    |  2040   |   1.051981   |     -      |     -     |   2.27   \n",
      "   7    |  2060   |   1.097746   |     -      |     -     |   2.26   \n",
      "   7    |  2080   |   1.158309   |     -      |     -     |   2.27   \n",
      "   7    |  2100   |   0.945346   |     -      |     -     |   2.27   \n",
      "   7    |  2120   |   1.261586   |     -      |     -     |   2.26   \n",
      "   7    |  2140   |   1.304226   |     -      |     -     |   2.27   \n",
      "   7    |  2160   |   1.391962   |     -      |     -     |   2.27   \n",
      "   7    |  2180   |   1.148779   |     -      |     -     |   2.26   \n",
      "   7    |  2200   |   1.334834   |     -      |     -     |   2.27   \n",
      "   7    |  2220   |   0.745652   |     -      |     -     |   2.27   \n",
      "   7    |  2240   |   1.267877   |     -      |     -     |   2.27   \n",
      "   7    |  2260   |   1.519289   |     -      |     -     |   2.28   \n",
      "   7    |  2280   |   1.447058   |     -      |     -     |   2.28   \n",
      "   7    |  2300   |   1.075148   |     -      |     -     |   2.28   \n",
      "   7    |  2320   |   1.198540   |     -      |     -     |   2.28   \n",
      "   7    |  2340   |   1.331107   |     -      |     -     |   2.28   \n",
      "   7    |  2360   |   0.935533   |     -      |     -     |   2.28   \n",
      "   7    |  2380   |   0.908413   |     -      |     -     |   2.27   \n",
      "   7    |  2400   |   1.432518   |     -      |     -     |   2.27   \n",
      "   7    |  2420   |   1.284099   |     -      |     -     |   2.28   \n",
      "   7    |  2440   |   1.192166   |     -      |     -     |   2.28   \n",
      "   7    |  2460   |   1.478130   |     -      |     -     |   2.28   \n",
      "   7    |  2480   |   1.265870   |     -      |     -     |   2.26   \n",
      "   7    |  2499   |   1.302948   |     -      |     -     |   2.12   \n",
      "----------------------------------------------------------------------\n",
      "   7    |    -    |   1.205201   |  1.330489  |   60.30   |  299.63  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   8    |   20    |   1.280920   |     -      |     -     |   2.44   \n",
      "   8    |   40    |   1.129088   |     -      |     -     |   2.27   \n",
      "   8    |   60    |   1.290745   |     -      |     -     |   2.27   \n",
      "   8    |   80    |   1.357911   |     -      |     -     |   2.27   \n",
      "   8    |   100   |   0.878792   |     -      |     -     |   2.27   \n",
      "   8    |   120   |   1.262833   |     -      |     -     |   2.27   \n",
      "   8    |   140   |   1.357368   |     -      |     -     |   2.27   \n",
      "   8    |   160   |   1.376297   |     -      |     -     |   2.27   \n",
      "   8    |   180   |   1.184229   |     -      |     -     |   2.27   \n",
      "   8    |   200   |   1.070038   |     -      |     -     |   2.27   \n",
      "   8    |   220   |   1.252025   |     -      |     -     |   2.26   \n",
      "   8    |   240   |   0.905655   |     -      |     -     |   2.28   \n",
      "   8    |   260   |   1.092584   |     -      |     -     |   2.26   \n",
      "   8    |   280   |   1.250058   |     -      |     -     |   2.27   \n",
      "   8    |   300   |   0.875606   |     -      |     -     |   2.27   \n",
      "   8    |   320   |   1.416702   |     -      |     -     |   2.27   \n",
      "   8    |   340   |   1.090353   |     -      |     -     |   2.27   \n",
      "   8    |   360   |   0.833371   |     -      |     -     |   2.27   \n",
      "   8    |   380   |   1.070690   |     -      |     -     |   2.27   \n",
      "   8    |   400   |   1.386026   |     -      |     -     |   2.28   \n",
      "   8    |   420   |   1.400205   |     -      |     -     |   2.27   \n",
      "   8    |   440   |   1.430788   |     -      |     -     |   2.27   \n",
      "   8    |   460   |   1.022860   |     -      |     -     |   2.27   \n",
      "   8    |   480   |   1.138791   |     -      |     -     |   2.27   \n",
      "   8    |   500   |   0.926559   |     -      |     -     |   2.26   \n",
      "   8    |   520   |   1.168098   |     -      |     -     |   2.27   \n",
      "   8    |   540   |   1.275121   |     -      |     -     |   2.27   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8    |   560   |   1.298262   |     -      |     -     |   2.26   \n",
      "   8    |   580   |   1.032891   |     -      |     -     |   2.26   \n",
      "   8    |   600   |   1.142039   |     -      |     -     |   2.26   \n",
      "   8    |   620   |   0.965496   |     -      |     -     |   2.27   \n",
      "   8    |   640   |   1.309241   |     -      |     -     |   2.28   \n",
      "   8    |   660   |   1.007299   |     -      |     -     |   2.27   \n",
      "   8    |   680   |   1.283798   |     -      |     -     |   2.28   \n",
      "   8    |   700   |   1.133599   |     -      |     -     |   2.28   \n",
      "   8    |   720   |   0.966055   |     -      |     -     |   2.28   \n",
      "   8    |   740   |   1.219687   |     -      |     -     |   2.26   \n",
      "   8    |   760   |   1.371940   |     -      |     -     |   2.27   \n",
      "   8    |   780   |   1.565833   |     -      |     -     |   2.27   \n",
      "   8    |   800   |   1.071660   |     -      |     -     |   2.27   \n",
      "   8    |   820   |   1.010322   |     -      |     -     |   2.27   \n",
      "   8    |   840   |   1.149629   |     -      |     -     |   2.27   \n",
      "   8    |   860   |   1.327907   |     -      |     -     |   2.27   \n",
      "   8    |   880   |   1.772561   |     -      |     -     |   2.27   \n",
      "   8    |   900   |   0.887441   |     -      |     -     |   2.27   \n",
      "   8    |   920   |   1.280269   |     -      |     -     |   2.26   \n",
      "   8    |   940   |   1.023457   |     -      |     -     |   2.28   \n",
      "   8    |   960   |   1.420449   |     -      |     -     |   2.27   \n",
      "   8    |   980   |   1.333781   |     -      |     -     |   2.27   \n",
      "   8    |  1000   |   1.700173   |     -      |     -     |   2.26   \n",
      "   8    |  1020   |   1.504509   |     -      |     -     |   2.27   \n",
      "   8    |  1040   |   1.110857   |     -      |     -     |   2.27   \n",
      "   8    |  1060   |   1.005578   |     -      |     -     |   2.26   \n",
      "   8    |  1080   |   1.119139   |     -      |     -     |   2.27   \n",
      "   8    |  1100   |   1.298814   |     -      |     -     |   2.27   \n",
      "   8    |  1120   |   1.292635   |     -      |     -     |   2.26   \n",
      "   8    |  1140   |   0.852824   |     -      |     -     |   2.27   \n",
      "   8    |  1160   |   1.328553   |     -      |     -     |   2.27   \n",
      "   8    |  1180   |   1.314637   |     -      |     -     |   2.28   \n",
      "   8    |  1200   |   1.502732   |     -      |     -     |   2.27   \n",
      "   8    |  1220   |   1.224492   |     -      |     -     |   2.27   \n",
      "   8    |  1240   |   1.172978   |     -      |     -     |   2.27   \n",
      "   8    |  1260   |   1.224764   |     -      |     -     |   2.27   \n",
      "   8    |  1280   |   1.145918   |     -      |     -     |   2.28   \n",
      "   8    |  1300   |   1.021715   |     -      |     -     |   2.28   \n",
      "   8    |  1320   |   1.121479   |     -      |     -     |   2.27   \n",
      "   8    |  1340   |   1.529281   |     -      |     -     |   2.27   \n",
      "   8    |  1360   |   1.308484   |     -      |     -     |   2.26   \n",
      "   8    |  1380   |   1.200850   |     -      |     -     |   2.26   \n",
      "   8    |  1400   |   1.089134   |     -      |     -     |   2.27   \n",
      "   8    |  1420   |   1.351308   |     -      |     -     |   2.28   \n",
      "   8    |  1440   |   1.389953   |     -      |     -     |   2.28   \n",
      "   8    |  1460   |   1.095672   |     -      |     -     |   2.27   \n",
      "   8    |  1480   |   0.765726   |     -      |     -     |   2.27   \n",
      "   8    |  1500   |   1.391850   |     -      |     -     |   2.27   \n",
      "   8    |  1520   |   1.054655   |     -      |     -     |   2.27   \n",
      "   8    |  1540   |   1.294878   |     -      |     -     |   2.26   \n",
      "   8    |  1560   |   1.318233   |     -      |     -     |   2.27   \n",
      "   8    |  1580   |   0.783920   |     -      |     -     |   2.27   \n",
      "   8    |  1600   |   1.120839   |     -      |     -     |   2.26   \n",
      "   8    |  1620   |   1.009089   |     -      |     -     |   2.27   \n",
      "   8    |  1640   |   1.231982   |     -      |     -     |   2.27   \n",
      "   8    |  1660   |   1.335435   |     -      |     -     |   2.27   \n",
      "   8    |  1680   |   1.316507   |     -      |     -     |   2.27   \n",
      "   8    |  1700   |   1.344209   |     -      |     -     |   2.27   \n",
      "   8    |  1720   |   1.820027   |     -      |     -     |   2.28   \n",
      "   8    |  1740   |   1.178919   |     -      |     -     |   2.28   \n",
      "   8    |  1760   |   1.432953   |     -      |     -     |   2.28   \n",
      "   8    |  1780   |   1.449522   |     -      |     -     |   2.27   \n",
      "   8    |  1800   |   1.071987   |     -      |     -     |   2.27   \n",
      "   8    |  1820   |   0.872122   |     -      |     -     |   2.27   \n",
      "   8    |  1840   |   0.938888   |     -      |     -     |   2.27   \n",
      "   8    |  1860   |   1.055601   |     -      |     -     |   2.27   \n",
      "   8    |  1880   |   0.889040   |     -      |     -     |   2.27   \n",
      "   8    |  1900   |   0.897851   |     -      |     -     |   2.27   \n",
      "   8    |  1920   |   1.151926   |     -      |     -     |   2.27   \n",
      "   8    |  1940   |   1.433139   |     -      |     -     |   2.28   \n",
      "   8    |  1960   |   1.265025   |     -      |     -     |   2.27   \n",
      "   8    |  1980   |   1.568743   |     -      |     -     |   2.27   \n",
      "   8    |  2000   |   1.226722   |     -      |     -     |   2.26   \n",
      "   8    |  2020   |   1.357507   |     -      |     -     |   2.27   \n",
      "   8    |  2040   |   1.420809   |     -      |     -     |   2.27   \n",
      "   8    |  2060   |   1.131464   |     -      |     -     |   2.26   \n",
      "   8    |  2080   |   1.082502   |     -      |     -     |   2.27   \n",
      "   8    |  2100   |   1.063444   |     -      |     -     |   2.27   \n",
      "   8    |  2120   |   1.360518   |     -      |     -     |   2.26   \n",
      "   8    |  2140   |   1.343787   |     -      |     -     |   2.27   \n",
      "   8    |  2160   |   1.355478   |     -      |     -     |   2.28   \n",
      "   8    |  2180   |   1.258917   |     -      |     -     |   2.28   \n",
      "   8    |  2200   |   1.054062   |     -      |     -     |   2.28   \n",
      "   8    |  2220   |   1.410145   |     -      |     -     |   2.28   \n",
      "   8    |  2240   |   1.277227   |     -      |     -     |   2.27   \n",
      "   8    |  2260   |   1.017909   |     -      |     -     |   2.28   \n",
      "   8    |  2280   |   1.030301   |     -      |     -     |   2.28   \n",
      "   8    |  2300   |   1.094325   |     -      |     -     |   2.26   \n",
      "   8    |  2320   |   1.420418   |     -      |     -     |   2.28   \n",
      "   8    |  2340   |   1.097342   |     -      |     -     |   2.27   \n",
      "   8    |  2360   |   0.948374   |     -      |     -     |   2.27   \n",
      "   8    |  2380   |   1.180944   |     -      |     -     |   2.28   \n",
      "   8    |  2400   |   1.264993   |     -      |     -     |   2.28   \n",
      "   8    |  2420   |   1.236182   |     -      |     -     |   2.28   \n",
      "   8    |  2440   |   1.458810   |     -      |     -     |   2.27   \n",
      "   8    |  2460   |   1.087618   |     -      |     -     |   2.28   \n",
      "   8    |  2480   |   1.076126   |     -      |     -     |   2.28   \n",
      "   8    |  2499   |   1.139254   |     -      |     -     |   2.13   \n",
      "----------------------------------------------------------------------\n",
      "   8    |    -    |   1.204793   |  1.330489  |   60.30   |  299.81  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   9    |   20    |   1.396206   |     -      |     -     |   2.47   \n",
      "   9    |   40    |   1.059553   |     -      |     -     |   2.26   \n",
      "   9    |   60    |   1.179747   |     -      |     -     |   2.26   \n",
      "   9    |   80    |   1.092552   |     -      |     -     |   2.26   \n",
      "   9    |   100   |   0.878187   |     -      |     -     |   2.27   \n",
      "   9    |   120   |   1.046775   |     -      |     -     |   2.27   \n",
      "   9    |   140   |   1.317326   |     -      |     -     |   2.27   \n",
      "   9    |   160   |   1.255213   |     -      |     -     |   2.27   \n",
      "   9    |   180   |   1.196336   |     -      |     -     |   2.26   \n",
      "   9    |   200   |   1.189091   |     -      |     -     |   2.27   \n",
      "   9    |   220   |   0.946291   |     -      |     -     |   2.27   \n",
      "   9    |   240   |   0.921338   |     -      |     -     |   2.26   \n",
      "   9    |   260   |   1.384655   |     -      |     -     |   2.27   \n",
      "   9    |   280   |   1.206007   |     -      |     -     |   2.26   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9    |   300   |   1.408080   |     -      |     -     |   2.27   \n",
      "   9    |   320   |   1.199149   |     -      |     -     |   2.27   \n",
      "   9    |   340   |   1.483760   |     -      |     -     |   2.27   \n",
      "   9    |   360   |   1.483297   |     -      |     -     |   2.28   \n",
      "   9    |   380   |   1.220738   |     -      |     -     |   2.27   \n",
      "   9    |   400   |   1.170271   |     -      |     -     |   2.28   \n",
      "   9    |   420   |   1.107427   |     -      |     -     |   2.27   \n",
      "   9    |   440   |   1.501627   |     -      |     -     |   2.27   \n",
      "   9    |   460   |   1.143090   |     -      |     -     |   2.26   \n",
      "   9    |   480   |   1.282559   |     -      |     -     |   2.26   \n",
      "   9    |   500   |   1.360938   |     -      |     -     |   2.27   \n",
      "   9    |   520   |   1.278271   |     -      |     -     |   2.27   \n",
      "   9    |   540   |   1.540562   |     -      |     -     |   2.27   \n",
      "   9    |   560   |   1.120850   |     -      |     -     |   2.27   \n",
      "   9    |   580   |   1.491550   |     -      |     -     |   2.27   \n",
      "   9    |   600   |   1.049557   |     -      |     -     |   2.27   \n",
      "   9    |   620   |   1.141787   |     -      |     -     |   2.27   \n",
      "   9    |   640   |   1.240196   |     -      |     -     |   2.26   \n",
      "   9    |   660   |   1.229688   |     -      |     -     |   2.26   \n",
      "   9    |   680   |   1.583786   |     -      |     -     |   2.26   \n",
      "   9    |   700   |   0.886109   |     -      |     -     |   2.27   \n",
      "   9    |   720   |   1.406087   |     -      |     -     |   2.27   \n",
      "   9    |   740   |   1.094205   |     -      |     -     |   2.26   \n",
      "   9    |   760   |   1.712508   |     -      |     -     |   2.27   \n",
      "   9    |   780   |   1.167538   |     -      |     -     |   2.27   \n",
      "   9    |   800   |   1.155745   |     -      |     -     |   2.27   \n",
      "   9    |   820   |   1.564156   |     -      |     -     |   2.27   \n",
      "   9    |   840   |   1.359517   |     -      |     -     |   2.27   \n",
      "   9    |   860   |   0.957962   |     -      |     -     |   2.27   \n",
      "   9    |   880   |   0.923695   |     -      |     -     |   2.27   \n",
      "   9    |   900   |   1.453315   |     -      |     -     |   2.27   \n",
      "   9    |   920   |   1.261807   |     -      |     -     |   2.27   \n",
      "   9    |   940   |   1.149132   |     -      |     -     |   2.26   \n",
      "   9    |   960   |   1.070964   |     -      |     -     |   2.28   \n",
      "   9    |   980   |   1.296903   |     -      |     -     |   2.28   \n",
      "   9    |  1000   |   1.281426   |     -      |     -     |   2.28   \n",
      "   9    |  1020   |   0.893651   |     -      |     -     |   2.28   \n",
      "   9    |  1040   |   1.411085   |     -      |     -     |   2.27   \n",
      "   9    |  1060   |   1.581594   |     -      |     -     |   2.27   \n",
      "   9    |  1080   |   1.278908   |     -      |     -     |   2.27   \n",
      "   9    |  1100   |   0.993876   |     -      |     -     |   2.27   \n",
      "   9    |  1120   |   1.492021   |     -      |     -     |   2.26   \n",
      "   9    |  1140   |   1.262518   |     -      |     -     |   2.27   \n",
      "   9    |  1160   |   1.404540   |     -      |     -     |   2.27   \n",
      "   9    |  1180   |   1.330710   |     -      |     -     |   2.27   \n",
      "   9    |  1200   |   1.159309   |     -      |     -     |   2.27   \n",
      "   9    |  1220   |   1.051732   |     -      |     -     |   2.27   \n",
      "   9    |  1240   |   0.964447   |     -      |     -     |   2.27   \n",
      "   9    |  1260   |   1.381354   |     -      |     -     |   2.27   \n",
      "   9    |  1280   |   1.078676   |     -      |     -     |   2.27   \n",
      "   9    |  1300   |   1.283636   |     -      |     -     |   2.27   \n",
      "   9    |  1320   |   1.130118   |     -      |     -     |   2.27   \n",
      "   9    |  1340   |   0.915998   |     -      |     -     |   2.27   \n",
      "   9    |  1360   |   1.317964   |     -      |     -     |   2.27   \n",
      "   9    |  1380   |   1.006512   |     -      |     -     |   2.27   \n",
      "   9    |  1400   |   1.220923   |     -      |     -     |   2.27   \n",
      "   9    |  1420   |   1.264313   |     -      |     -     |   2.28   \n",
      "   9    |  1440   |   1.131773   |     -      |     -     |   2.27   \n",
      "   9    |  1460   |   1.089355   |     -      |     -     |   2.27   \n",
      "   9    |  1480   |   0.948893   |     -      |     -     |   2.27   \n",
      "   9    |  1500   |   1.158439   |     -      |     -     |   2.27   \n",
      "   9    |  1520   |   1.149257   |     -      |     -     |   2.27   \n",
      "   9    |  1540   |   0.780374   |     -      |     -     |   2.27   \n",
      "   9    |  1560   |   1.123673   |     -      |     -     |   2.28   \n",
      "   9    |  1580   |   0.988186   |     -      |     -     |   2.27   \n",
      "   9    |  1600   |   1.247613   |     -      |     -     |   2.27   \n",
      "   9    |  1620   |   1.307862   |     -      |     -     |   2.27   \n",
      "   9    |  1640   |   0.906511   |     -      |     -     |   2.27   \n",
      "   9    |  1660   |   1.105862   |     -      |     -     |   2.28   \n",
      "   9    |  1680   |   1.111238   |     -      |     -     |   2.27   \n",
      "   9    |  1700   |   1.316175   |     -      |     -     |   2.26   \n",
      "   9    |  1720   |   1.262188   |     -      |     -     |   2.27   \n",
      "   9    |  1740   |   1.582713   |     -      |     -     |   2.27   \n",
      "   9    |  1760   |   1.333186   |     -      |     -     |   2.27   \n",
      "   9    |  1780   |   0.966355   |     -      |     -     |   2.27   \n",
      "   9    |  1800   |   1.031839   |     -      |     -     |   2.27   \n",
      "   9    |  1820   |   0.984697   |     -      |     -     |   2.27   \n",
      "   9    |  1840   |   1.096135   |     -      |     -     |   2.27   \n",
      "   9    |  1860   |   1.497798   |     -      |     -     |   2.27   \n",
      "   9    |  1880   |   0.953973   |     -      |     -     |   2.27   \n",
      "   9    |  1900   |   1.193124   |     -      |     -     |   2.27   \n",
      "   9    |  1920   |   1.232208   |     -      |     -     |   2.28   \n",
      "   9    |  1940   |   1.277041   |     -      |     -     |   2.28   \n",
      "   9    |  1960   |   1.376320   |     -      |     -     |   2.28   \n",
      "   9    |  1980   |   0.946674   |     -      |     -     |   2.28   \n",
      "   9    |  2000   |   0.808980   |     -      |     -     |   2.27   \n",
      "   9    |  2020   |   1.522213   |     -      |     -     |   2.27   \n",
      "   9    |  2040   |   1.135671   |     -      |     -     |   2.26   \n",
      "   9    |  2060   |   1.391490   |     -      |     -     |   2.27   \n",
      "   9    |  2080   |   1.267585   |     -      |     -     |   2.26   \n",
      "   9    |  2100   |   0.929221   |     -      |     -     |   2.26   \n",
      "   9    |  2120   |   1.583983   |     -      |     -     |   2.28   \n",
      "   9    |  2140   |   1.178281   |     -      |     -     |   2.28   \n",
      "   9    |  2160   |   1.255431   |     -      |     -     |   2.28   \n",
      "   9    |  2180   |   1.043799   |     -      |     -     |   2.28   \n",
      "   9    |  2200   |   0.974879   |     -      |     -     |   2.28   \n",
      "   9    |  2220   |   0.796017   |     -      |     -     |   2.27   \n",
      "   9    |  2240   |   1.043912   |     -      |     -     |   2.27   \n",
      "   9    |  2260   |   1.522131   |     -      |     -     |   2.27   \n",
      "   9    |  2280   |   1.404268   |     -      |     -     |   2.27   \n",
      "   9    |  2300   |   1.172136   |     -      |     -     |   2.27   \n",
      "   9    |  2320   |   1.184692   |     -      |     -     |   2.26   \n",
      "   9    |  2340   |   0.932885   |     -      |     -     |   2.26   \n",
      "   9    |  2360   |   1.437734   |     -      |     -     |   2.27   \n",
      "   9    |  2380   |   1.198084   |     -      |     -     |   2.27   \n",
      "   9    |  2400   |   1.142266   |     -      |     -     |   2.28   \n",
      "   9    |  2420   |   1.273358   |     -      |     -     |   2.27   \n",
      "   9    |  2440   |   1.666284   |     -      |     -     |   2.27   \n",
      "   9    |  2460   |   1.353486   |     -      |     -     |   2.26   \n",
      "   9    |  2480   |   0.809771   |     -      |     -     |   2.27   \n",
      "   9    |  2499   |   1.240985   |     -      |     -     |   2.12   \n",
      "----------------------------------------------------------------------\n",
      "   9    |    -    |   1.205185   |  1.330489  |   60.30   |  299.77  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "  10    |   20    |   1.434882   |     -      |     -     |   2.46   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10    |   40    |   1.770478   |     -      |     -     |   2.26   \n",
      "  10    |   60    |   1.244153   |     -      |     -     |   2.27   \n",
      "  10    |   80    |   0.882368   |     -      |     -     |   2.27   \n",
      "  10    |   100   |   1.023933   |     -      |     -     |   2.27   \n",
      "  10    |   120   |   0.913606   |     -      |     -     |   2.26   \n",
      "  10    |   140   |   1.176484   |     -      |     -     |   2.26   \n",
      "  10    |   160   |   1.269993   |     -      |     -     |   2.27   \n",
      "  10    |   180   |   1.133634   |     -      |     -     |   2.27   \n",
      "  10    |   200   |   1.280986   |     -      |     -     |   2.26   \n",
      "  10    |   220   |   1.137322   |     -      |     -     |   2.26   \n",
      "  10    |   240   |   1.297229   |     -      |     -     |   2.26   \n",
      "  10    |   260   |   1.205506   |     -      |     -     |   2.26   \n",
      "  10    |   280   |   1.853442   |     -      |     -     |   2.27   \n",
      "  10    |   300   |   1.104846   |     -      |     -     |   2.28   \n",
      "  10    |   320   |   0.929494   |     -      |     -     |   2.28   \n",
      "  10    |   340   |   1.206349   |     -      |     -     |   2.28   \n",
      "  10    |   360   |   0.797722   |     -      |     -     |   2.28   \n",
      "  10    |   380   |   1.065170   |     -      |     -     |   2.28   \n",
      "  10    |   400   |   1.147052   |     -      |     -     |   2.27   \n",
      "  10    |   420   |   1.179107   |     -      |     -     |   2.27   \n",
      "  10    |   440   |   1.156143   |     -      |     -     |   2.26   \n",
      "  10    |   460   |   1.110870   |     -      |     -     |   2.26   \n",
      "  10    |   480   |   1.246791   |     -      |     -     |   2.27   \n",
      "  10    |   500   |   1.127692   |     -      |     -     |   2.26   \n",
      "  10    |   520   |   1.358046   |     -      |     -     |   2.27   \n",
      "  10    |   540   |   0.661986   |     -      |     -     |   2.26   \n",
      "  10    |   560   |   1.018583   |     -      |     -     |   2.27   \n",
      "  10    |   580   |   1.179816   |     -      |     -     |   2.26   \n",
      "  10    |   600   |   0.859015   |     -      |     -     |   2.27   \n",
      "  10    |   620   |   1.142597   |     -      |     -     |   2.27   \n",
      "  10    |   640   |   1.210628   |     -      |     -     |   2.27   \n",
      "  10    |   660   |   0.473899   |     -      |     -     |   2.27   \n",
      "  10    |   680   |   1.049603   |     -      |     -     |   2.26   \n",
      "  10    |   700   |   1.135198   |     -      |     -     |   2.26   \n",
      "  10    |   720   |   0.938357   |     -      |     -     |   2.26   \n",
      "  10    |   740   |   0.989935   |     -      |     -     |   2.27   \n",
      "  10    |   760   |   1.289881   |     -      |     -     |   2.28   \n",
      "  10    |   780   |   1.357501   |     -      |     -     |   2.27   \n",
      "  10    |   800   |   0.940724   |     -      |     -     |   2.27   \n",
      "  10    |   820   |   0.775713   |     -      |     -     |   2.27   \n",
      "  10    |   840   |   1.253483   |     -      |     -     |   2.27   \n",
      "  10    |   860   |   1.170339   |     -      |     -     |   2.27   \n",
      "  10    |   880   |   1.462874   |     -      |     -     |   2.27   \n",
      "  10    |   900   |   1.324953   |     -      |     -     |   2.27   \n",
      "  10    |   920   |   1.303182   |     -      |     -     |   2.27   \n",
      "  10    |   940   |   1.323393   |     -      |     -     |   2.26   \n",
      "  10    |   960   |   1.187104   |     -      |     -     |   2.26   \n",
      "  10    |   980   |   1.276431   |     -      |     -     |   2.26   \n",
      "  10    |  1000   |   1.482426   |     -      |     -     |   2.26   \n",
      "  10    |  1020   |   1.262438   |     -      |     -     |   2.26   \n",
      "  10    |  1040   |   0.819511   |     -      |     -     |   2.27   \n",
      "  10    |  1060   |   1.160432   |     -      |     -     |   2.28   \n",
      "  10    |  1080   |   1.281833   |     -      |     -     |   2.28   \n",
      "  10    |  1100   |   1.020506   |     -      |     -     |   2.27   \n",
      "  10    |  1120   |   1.045846   |     -      |     -     |   2.27   \n",
      "  10    |  1140   |   1.076492   |     -      |     -     |   2.27   \n",
      "  10    |  1160   |   1.611430   |     -      |     -     |   2.27   \n",
      "  10    |  1180   |   1.531027   |     -      |     -     |   2.27   \n",
      "  10    |  1200   |   0.953013   |     -      |     -     |   2.27   \n",
      "  10    |  1220   |   1.521247   |     -      |     -     |   2.27   \n",
      "  10    |  1240   |   1.187041   |     -      |     -     |   2.27   \n",
      "  10    |  1260   |   1.352838   |     -      |     -     |   2.27   \n",
      "  10    |  1280   |   0.800069   |     -      |     -     |   2.29   \n",
      "  10    |  1300   |   1.179898   |     -      |     -     |   2.28   \n",
      "  10    |  1320   |   1.112481   |     -      |     -     |   2.28   \n",
      "  10    |  1340   |   1.197611   |     -      |     -     |   2.28   \n",
      "  10    |  1360   |   1.309368   |     -      |     -     |   2.28   \n",
      "  10    |  1380   |   1.324506   |     -      |     -     |   2.27   \n",
      "  10    |  1400   |   0.930447   |     -      |     -     |   2.27   \n",
      "  10    |  1420   |   1.189472   |     -      |     -     |   2.26   \n",
      "  10    |  1440   |   0.724956   |     -      |     -     |   2.27   \n",
      "  10    |  1460   |   1.355091   |     -      |     -     |   2.26   \n",
      "  10    |  1480   |   0.818204   |     -      |     -     |   2.28   \n",
      "  10    |  1500   |   1.184982   |     -      |     -     |   2.26   \n",
      "  10    |  1520   |   1.039418   |     -      |     -     |   2.28   \n",
      "  10    |  1540   |   1.095507   |     -      |     -     |   2.27   \n",
      "  10    |  1560   |   1.724845   |     -      |     -     |   2.26   \n",
      "  10    |  1580   |   1.200723   |     -      |     -     |   2.27   \n",
      "  10    |  1600   |   0.923381   |     -      |     -     |   2.27   \n",
      "  10    |  1620   |   1.052081   |     -      |     -     |   2.27   \n",
      "  10    |  1640   |   0.920138   |     -      |     -     |   2.27   \n",
      "  10    |  1660   |   1.452696   |     -      |     -     |   2.27   \n",
      "  10    |  1680   |   1.108018   |     -      |     -     |   2.27   \n",
      "  10    |  1700   |   1.432732   |     -      |     -     |   2.28   \n",
      "  10    |  1720   |   1.125182   |     -      |     -     |   2.28   \n",
      "  10    |  1740   |   1.163647   |     -      |     -     |   2.27   \n",
      "  10    |  1760   |   1.138940   |     -      |     -     |   2.27   \n",
      "  10    |  1780   |   1.301727   |     -      |     -     |   2.27   \n",
      "  10    |  1800   |   1.139095   |     -      |     -     |   2.27   \n",
      "  10    |  1820   |   1.039902   |     -      |     -     |   2.27   \n",
      "  10    |  1840   |   1.187877   |     -      |     -     |   2.28   \n",
      "  10    |  1860   |   0.797891   |     -      |     -     |   2.28   \n",
      "  10    |  1880   |   1.627932   |     -      |     -     |   2.28   \n",
      "  10    |  1900   |   1.609496   |     -      |     -     |   2.28   \n",
      "  10    |  1920   |   1.210711   |     -      |     -     |   2.29   \n",
      "  10    |  1940   |   1.188990   |     -      |     -     |   2.28   \n",
      "  10    |  1960   |   1.685074   |     -      |     -     |   2.28   \n",
      "  10    |  1980   |   1.463865   |     -      |     -     |   2.28   \n",
      "  10    |  2000   |   1.268575   |     -      |     -     |   2.28   \n",
      "  10    |  2020   |   1.235028   |     -      |     -     |   2.28   \n",
      "  10    |  2040   |   0.794661   |     -      |     -     |   2.27   \n",
      "  10    |  2060   |   1.304949   |     -      |     -     |   2.27   \n",
      "  10    |  2080   |   1.547220   |     -      |     -     |   2.27   \n",
      "  10    |  2100   |   1.066902   |     -      |     -     |   2.27   \n",
      "  10    |  2120   |   1.133588   |     -      |     -     |   2.27   \n",
      "  10    |  2140   |   1.409347   |     -      |     -     |   2.27   \n",
      "  10    |  2160   |   1.117183   |     -      |     -     |   2.27   \n",
      "  10    |  2180   |   1.085948   |     -      |     -     |   2.27   \n",
      "  10    |  2200   |   1.539652   |     -      |     -     |   2.28   \n",
      "  10    |  2220   |   1.245914   |     -      |     -     |   2.28   \n",
      "  10    |  2240   |   1.591163   |     -      |     -     |   2.27   \n",
      "  10    |  2260   |   0.932563   |     -      |     -     |   2.27   \n",
      "  10    |  2280   |   1.516034   |     -      |     -     |   2.26   \n",
      "  10    |  2300   |   1.261433   |     -      |     -     |   2.26   \n",
      "  10    |  2320   |   1.439406   |     -      |     -     |   2.27   \n",
      "  10    |  2340   |   0.941578   |     -      |     -     |   2.28   \n",
      "  10    |  2360   |   1.270921   |     -      |     -     |   2.27   \n",
      "  10    |  2380   |   1.401021   |     -      |     -     |   2.27   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10    |  2400   |   1.391136   |     -      |     -     |   2.27   \n",
      "  10    |  2420   |   1.893721   |     -      |     -     |   2.27   \n",
      "  10    |  2440   |   1.091594   |     -      |     -     |   2.28   \n",
      "  10    |  2460   |   1.906515   |     -      |     -     |   2.28   \n",
      "  10    |  2480   |   1.529574   |     -      |     -     |   2.28   \n",
      "  10    |  2499   |   1.208236   |     -      |     -     |   2.13   \n",
      "----------------------------------------------------------------------\n",
      "  10    |    -    |   1.204614   |  1.330489  |   60.30   |  299.89  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=10, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xPbbDrzeiwUj"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LpkLVKIJlYWy"
   },
   "outputs": [],
   "source": [
    "# # Concatenate the train set and the validation set\n",
    "# full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "# full_train_sampler = RandomSampler(full_train_data)\n",
    "# full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
    "\n",
    "# # Train the Bert Classifier on the entire training data\n",
    "# set_seed(42)\n",
    "# bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "# train(bert_classifier, full_train_dataloader, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIz4OzAPlc4Z"
   },
   "source": [
    "## Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1mrY2M5jlevk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(df_test['Text'])\n",
    "\n",
    "test_labels = torch.tensor((df_test['Score']-1).to_numpy())\n",
    "\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3138304194509982\n",
      "Test Accuracy 61.6\n"
     ]
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "test_loss, test_accuracy = evaluate(bert_classifier, test_dataloader)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A4T2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c5e076fb618145f5fc0ee75211ab0cef3f23db70858170546aaefb49107fdf35"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1655c36f82c840278690f1070561b072": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e510cb422654cc7af47d5fe066c5827": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29f751fc3d35416b88a2d6a524578b63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_487f9e5fe119446a897a3a95ac38c65d",
       "IPY_MODEL_a430ffcaba4145439221bddadc477815",
       "IPY_MODEL_aed444330bf64b29aa99f43fa835473e"
      ],
      "layout": "IPY_MODEL_7f163878271e48f1a758bc24d43e5978"
     }
    },
    "3d79ec161c8b4336b1daf0edb9235215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dda98c8377844138d4624931f054491",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7c50bf70806c478fa48df1df201ec733",
      "value": 28
     }
    },
    "3dda98c8377844138d4624931f054491": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "452f0140f7114b91adbfa09ecb3fb56c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "487f9e5fe119446a897a3a95ac38c65d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aebeca6d11564533b0e2885028ac2c32",
      "placeholder": "​",
      "style": "IPY_MODEL_8243c1bcf44146b999b03420bb2b8e66",
      "value": "Downloading: 100%"
     }
    },
    "4913a94fa16044e6b243aebcb64e6a31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ba590880552402b97ae6c43a1b2d3c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e526c559a5bb4ec1ad552ada9a7a8958",
      "placeholder": "​",
      "style": "IPY_MODEL_8c919b6325354ec4abddc7aaebb445b4",
      "value": "Downloading: 100%"
     }
    },
    "50f6cf48e9584298bbc1222cae097058": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "599dce4954174f2b8909015d4c377290": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8330658fd0824df29ba8e8450b787497",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abdfcfab69364a65a8233b0389266d01",
      "value": 231508
     }
    },
    "5b9737421df24fe0bdde038e8e14e8b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1b6df08e9b24e96a67a995509a2e10d",
      "placeholder": "​",
      "style": "IPY_MODEL_fe73813b431e4475bdb0972f16815b2a",
      "value": " 28.0/28.0 [00:00&lt;00:00, 707B/s]"
     }
    },
    "5e767bd5985044f888d84bd31230738c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_affa2a97c9c245d59365f74bce6b9772",
      "placeholder": "​",
      "style": "IPY_MODEL_af440b9acaff473e8269ae3944d693e9",
      "value": " 226k/226k [00:00&lt;00:00, 2.41MB/s]"
     }
    },
    "7959978aa50b4ebf9441f02615f1294d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c50bf70806c478fa48df1df201ec733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f163878271e48f1a758bc24d43e5978": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8243c1bcf44146b999b03420bb2b8e66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8330658fd0824df29ba8e8450b787497": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c919b6325354ec4abddc7aaebb445b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f82b402fd344929814a2dce4aacf700": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4ba590880552402b97ae6c43a1b2d3c5",
       "IPY_MODEL_3d79ec161c8b4336b1daf0edb9235215",
       "IPY_MODEL_5b9737421df24fe0bdde038e8e14e8b4"
      ],
      "layout": "IPY_MODEL_1655c36f82c840278690f1070561b072"
     }
    },
    "93e47265d4994aa789abed0d50def9b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e2b6e0cd20794e22940abeeb42cc8e96",
       "IPY_MODEL_599dce4954174f2b8909015d4c377290",
       "IPY_MODEL_5e767bd5985044f888d84bd31230738c"
      ],
      "layout": "IPY_MODEL_7959978aa50b4ebf9441f02615f1294d"
     }
    },
    "a430ffcaba4145439221bddadc477815": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e510cb422654cc7af47d5fe066c5827",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4913a94fa16044e6b243aebcb64e6a31",
      "value": 570
     }
    },
    "abdfcfab69364a65a8233b0389266d01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aebeca6d11564533b0e2885028ac2c32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aed444330bf64b29aa99f43fa835473e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7385c515c054a41be7896dbec63c823",
      "placeholder": "​",
      "style": "IPY_MODEL_50f6cf48e9584298bbc1222cae097058",
      "value": " 570/570 [00:00&lt;00:00, 14.3kB/s]"
     }
    },
    "af440b9acaff473e8269ae3944d693e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "affa2a97c9c245d59365f74bce6b9772": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c13f3c74b4804dc89c5c89ed9b81b202": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1b6df08e9b24e96a67a995509a2e10d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7385c515c054a41be7896dbec63c823": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2b6e0cd20794e22940abeeb42cc8e96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_452f0140f7114b91adbfa09ecb3fb56c",
      "placeholder": "​",
      "style": "IPY_MODEL_c13f3c74b4804dc89c5c89ed9b81b202",
      "value": "Downloading: 100%"
     }
    },
    "e526c559a5bb4ec1ad552ada9a7a8958": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe73813b431e4475bdb0972f16815b2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
